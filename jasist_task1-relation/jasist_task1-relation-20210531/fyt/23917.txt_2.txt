Indeed , the consistency of references is an important aspect since , for instance , Belter ( 2014 ) reported cases where there are several hundred variants of citations to the same data set ( i.e. , the World Ocean Atlas and World Ocean Database ) . 
Borgman ( 2012b ) and NSF ( 2015 ) stressed the importance of developing shared and extensible metadata formats for data citation . 
The last JDDCP principle in the first group is interoperability , which requires that data citation methods are flexible enough to operate through different communities and citation practices . 
The second group of JDDCP principles defines some of the requisites for data citation methods and systems ; it is composed of unique identification , access , persistence , and specificity . 
Unique identification indicates that a citation system must provide a methodology for identification that is machine‐actionable and widely used within the community of reference . 
This principle is connected to : 1 ) persistence , which states that unique identifiers should persist even after the lifespan of data they are associated with ; and 2 ) specificity , which states that we should be able to identify specific data ( even a data subset ) to support a claim and that such data should be connected to provenance information useful for reconstructing their context . 
Persistence or fixity is a key factor for data citation because a data citation system should guarantee that a cited object is always available in the cited form across time . 
Ensuring the persistence of a database may not be enough ( Thorisson , 2009 ) , because to guarantee fixity of data citation we need database versioning systems ; indeed , maintaining only the latest version of a data set is not sufficient , given that cited data may change or become unavailable across time without the knowledge of those who are citing it ( Borgman , 2012b ; Pröll & Rauber , 2013 ; Rauber , Ari , van Uytvanck , & Pröll , 2016 ) . 
In a context where citation to big and evolving data sets has to be created and maintained over time , the requirements a data citation method should respect are becoming more demanding and complex than those defined by JDDCP . 
Unique identification is the most general and widely recognized requirement of a data citation method ( Fenner et al. , 2016 ) , but the use of persistent identifiers ( PID ) such as the Digital Object Identifier ( DOI ) has been highlighted as not being the “ magic bullet ” able to solve all the identification problems because we need to be able to uniquely identify subsets of data with variable granularity ( Kafkas et al. , 2015 ; Van de Sompel , 2012 ; Crosas et al. , 2015 ) since different fields require different levels of detail to be able to reproduce data ; in other cases , we may need to identify sets of data aggregated from different sources . 
The granularity problem can not be addressed by the use of data papers as proxies for the data to be cited ( Candela , Castelli , Manghi , & Tani , 2015 ) , since they can only be used for citing a data set as a whole ; the same issue affects widely used online resources , which associate a PID to a data set and make it available and de‐referenceable online , but do not provide variable granularity access and identification of subsets of the data . 
This problem introduces both theoretical and practical ( further discussed below ) issues that exist also for textual citations . 
Indeed , in the context of traditional publications there are cases where it is customary to cite entire documents ( e.g. , scientific practice ) or to cite single pages or passages ( e.g. , in the humanities ) . 
Nevertheless , in bibliometrics the convention is to aggregate documents by common elements ( e.g. , author or journal ) and the unit of analysis is often the cited document ( Borgman , 1990 ) . 
On the contrary , for data citation there is not an agreed upon and shared protocol to be followed , and being able to identify and cite a subset of data is a central problem that requires further research . 
From the analyses conducted so far , we can state that the ideal data citation system should uniquely identify a data set and subsets of it with different levels of coarseness ( identification ) , attribute the ownership and responsibility of the data with variable granularity to the right people/institutions ( attribution ) , guarantee the persistence of the data being cited as well as the citations themselves ( fixity ) , and automatically create complete and consistent citation snippets ( completeness and consistency ) according to community practices and shared metadata standards . 
This citation system should be flexible enough to accommodate diverse requirements and citation practices across different disciplines as well as heterogeneous data models such as the relational model , the XML hierarchical model , or the RDF graph model . 
From case to case , the objects being cited within a given data set may vary considerably ; indeed , a data citation system needs to be able to cite : A single resource such as a complete data set or a specific resource within a data set ; for example , an RDF resource identified by a URI , an XML node identified by a single path from the tree root , a single tuple within a table in a relational database . 
A subset/selection of resources such as a selection of resources or portions of resources within a data set ; for example , a bunch of RDF statements ( e.g. , a subgraph of an RDF data set ) , an XML subtree , or a set of tuples selected from a relational table . 
An aggregation of resources such as the union or joining of resources coming from different parts of a data set or even different data sets ; for example , an RDF graph created by joining together two RDF subgraphs , a bunch of XML subtrees spanning one or more files , a set of relational tuples obtained by joining tables within the same database , or from different databases . 
In Table 2 , we report the main characteristics of the data citation systems and methods proposed in the literature according to the aspects outlined above . 
We highlight how these methods address the problem of identification , the automatic creation of citation snippets ( i.e. , automatic or manual ) , the coarseness of the produced citations ( i.e. , single resources , subsets or aggregations ) , and if these methods are general or defined for specific data models . 
Any data type . 
CSV and Relational DB implementation . 
Most of the solutions proposed in the literature use PID for addressing the identification problem . 
As we highlighted above , the use of PID is a viable solution for citing a data set as a whole , but it is not straightforward to use them for citing subsets or aggregations of data . 
In several cases , identification is provided to a high level of coarseness and identifying data with variable granularity is not possible . 
For instance , Bandrowski et al . 
( 2015 ) promoted the use of Research Resource Identifiers ( RRID ) , which are unique persistent identifiers assigned to scientific resources . 
In particular , they have been used with biological resources such as antibodies , model organisms , and tools ( software , databases , services ) . 
RRID are meant to identify resources at a high level of granularity and “ for software and databases , we elected to identify just the root entity and not a granular citation of a particular software version or database. ” Van de Sompel ( 2012 ) outlines two options to address the identification problem : to mint a new PID for each segment of data that need to be cited , or to mint a single PID for the data set and to store the user query selecting or aggregating data . 
The first solution has been chosen by DataVerse ( Crosas , 2011 ) , which assigns a DOI to a data set and generates a UNF ( Universal Numerical Fingerprint ) for each data segment to be cited within the data set . 
A UNF is a PID assigned to specific digital objects or subsets within a data set and it allows for identifying data with different granularities . 
This system needs to be extended/revised to handle the citations of big dynamic data in order to be able to cite a subset of data on 1 ) selected variables and observations for large quantitative data ; 2 ) time‐stamp intervals ; and , 3 ) spatial dimensions ( Crosas et al. , 2015 ) and also to be able to handle queries to the data . 
The use of DOI raised some concerns about the cost of minting identifiers for large data archives ( Henderson & Kotz , 2015 ) . 
Buneman and Silvello ( 2010 ) and Silvello ( 2017 ) discussed the case of XML data , where every single node of an XML file can be a citable unit . 
In this case , a PID can be assigned to every XML file in a collection , but assigning a PID to every node would be unsustainable , if not unfeasible . 
Buneman and Silvello ( 2010 ) proposed identifying a single node within an XML file by associating the PID identifying the XML file with the unique key identifying a specific node within the given XML file—for example , the path of the node from the root of the file . 
Silvello ( 2015 ) discussed the case of RDF data sets , where we may need to cite a set of RDF statements ( e.g. , a subgraph of the whole RDF data set at hand ) . 
In this case , every single node in an RDF graph is uniquely identified by a URI ( i.e. , a PID ) , but we can not possibly assign a URI for every possible aggregation of nodes or statement in a data set . 
The proposed solution is based on the use of named graphs ( Klyne , Carroll , & McBride , 2014 ) , which are a set of connected RDF statements identified by a global PID—that is , a URI , which is the name of the graph . 
Groth et al . 
( 2010 ) proposed the nanopublication mode , l where research data that may need to be cited ( or for which provenance and attribution information have to be held for any other motive ) are modeled as single statements in a subject‐property‐object form , as in an RDF triple . 
The RDF triple is uniquely identified by a URI associated with the triple as in named graphs . 
The nanopublication model defines the minimum citable unit to be a triple ; even though it is not straightforwardly extensible to identify an aggregation of statements , the nanopublication model is widely used by the bioinformatics research community ( Clark , Ciccarese , & Goble , 2014 ; Mina et al. , 2015 ) and there are some studies about its adoption in the humanities ( Gradmann , 2014 ; Golden & Shaw , 2016 ) . 
Pröll and Rauber ( 2013 ) and Rauber et al . 
( 2016 ) proposed an identification method based on assigning PID to queries , which are used as proxies for the data subset to be cited . 
The access to a data subset is allowed by reissuing the stored query and a citation is associated with the PID of the query identifying the data . 
This method is flexible and in principle works for any data format and model ; it requires the development of an additional infrastructure for storing the queries and assigning a timestamp to each query on the basis of the last update of the whole database . 
Moreover , it requires the development of some methods to guarantee query uniqueness—the proposal is to rewrite queries to a normalized form and then to compute a checksum to detect identical queries—and stable sorting to ensure that the sorting of the records in the returned data set is unambiguous and reproducible . 
This method has been developed in the context of the RDA ( RDA , 2016 ) and it has been implemented by several scientific data sets , as detailed in RDA ( 2016 ) . 
Parsons ( 2012 ) analyzed the problem of identifying subsets of a given data set in order to make them citable , stating that identifying the query with a PID and citing the query , or assigning a UNF to a data subset to be cited , as in the DataVerse model , are not viable solutions in the context of Earth Science , where there are no queries to be used and there is no agreed upon canonical version for the data . 
The solution adopted in this field is to use spatial and temporal information ( i.e. , a structural index , as they call it ) to identify a specific subset , since in Earth Science these data are always available and are often sufficient to discriminate between two subsets of a given data set . 
Hence , Klump , Huber , and Diepenbroek ( 2016 , p. 6 ) discussed the use of PIDs to identify and cite time series and evolving data sets as in the case of Earth Science resources where “ time or space are dimensions of the data and a subset may be defined by a range or bounding parameters . 
” The solution proposed is to enrich a PID with “ fragment identifiers ” in the form  @  where the “ fragment identifier ” can be the version of a resource or the time interval to be considered . 
The Virtual Atomic and Molecular Data Centre ( VAMDC ) ( Zwölf , Moreau , & Dubernet , 2016 ) foresees a hybrid solution for citing resource aggregations generated by issuing SQL queries to a federated database putting together many heterogeneous data sources . 
For identification purposes , VAMDC stores the user queries as proxies for the data , but instead of using the query PID for the citations , it creates a file containing the extracted data , information about the databases ( e.g. , version and contributors ) , and bibliographical information about the scientific papers related to the data ( i.e. , a form of provenance ) . 
This file is associated with a Digital Unique Identifier ( DUI ) , which points to a landing page containing all the information in the file . 
The idea is to use the DUI associated to the file generated for the user query as a citation and to use the landing page to get the information usually provided by the citation snippet . 
In the context of neuroimaging , Honor et al . 
( 2016 ) addressed the problem of citing an aggregation of resources ( i.e. , images ) created on the fly by the users . 
Here , there are three levels of identification : the image level where each resource is identified by an individual PID ( i.e. , DOI ) , the project level where predefined sets of images are assigned with a PID , and the functional level , where aggregations of resources defined on the fly by users are identified by a newly minted PID . 
This solution requires providomg a module for PID deduplication when two different PIDs are assigned to the same functional aggregation of resources . 
One goal in automating data citations is to structure them in a standardized format so that they can be formatted according to predefined styles and they can be included in bibliography management tools . 
There are several metadata formats for data citations ( Altman & King , 2007 ; Green , 2010 ; Starr & Gastl , 2011 ) and they all share a common subset of elements ( Ball & Duke , 2015 ) : author , publication date , title , edition , version , URI , resource type , publisher , unique number fingerprint ( a form of hash of the data ) , a persistent URL , and location . 
DataCite ( 2016 ) , a widely‐recognized metadata format proposal for citing data , expands this common set of fields by adding others such as subject , contributor , format , size , description , language , rights , and funding reference . 
DataCite is the citation format adopted by the Thompson Reuters DCI . 
Parsons ( 2012 ) stated that there are some differences between the DataCite schema and the metadata fields required for citing Earth Science Information Partners ( ESIP ) data : Authors , Release Date , Title , Version , Editors , Archive and/or Distributor , Locator , Date , and Time accessed , Subset used . 
The main difference regards the specification of the subset of data being used . 
Starr et al . 
( 2015 ) reports a different minimum set of metadata fields to be used to cite a data set : Dataset Identifier , Title , Description , Creator , Publisher/Contact , PublicationDate/Year/ReleaseDate , Version , Creator Identifier ( s ) ( optional ) , License ( optional ) . 
The Repository Early Adopters Experts Group ( an initiative of FORCE11 and the NIH BioCADDIE66 https : //biocaddie.org/ program ) ( Fenner et al. , 2016 ) stated that the minimum set of required metadata is Dataset Identifier , Title , Creator , Data Repository , Publication Date , Version , and Type . 
Bravo et al . 
( 2015 ) proposed an even smaller group of mandatory fields for citing biological resources : Identification ( ID and/or DOI ) , Bioresource name , Acronym ( if available ) ; ( if applicable ) Organization ; Number of accesses/Date of last access . 
By contrast , in the field of biodiversity Chavan ( 2012 ) underlined the need to develop two different metadata schemas , one related to published data set citations and another for query‐based citations ; he also proposed six alternative data citation styles . 
It is evident that one size does not fit all when it comes to metadata formats for data citation . 
Metadata formats for “ traditional ” resources in the digital library domain such as the well‐known Dublin Core adopted a dynamic solution that could be a viable possibility also in the data citation context . 
Indeed , the Dublin Core metadata standard is composed of 15 base elements—that is , the Simple Dublin Core—that can be extended by using the so‐called “ application profiles ” that adapt the standard to the requirements of specific application domains ( although an application profile is not only a set of extra metadata fields , it also comprises policies and guidelines defined for a particular application , implementation , or object type ) . 
In a similar vein , DataVerse ( Crosas , 2011 ) proposed to use the Altman and King ( 2007 ) metadata schema based on a minimal set of elements which can be extended from case to case . 
Metadata schemas should also be employed to enable the machine readability of data citations . 
The FORCE11 Data Citation Implementation Group in 2014 focused on this aspect by proposing to extend the NISO Z39.96‐2012 JATS XML standard for exchanging journal article content in a machine‐readable fashion ( Mietchen , McEntyre , Beck , & Maloney , 2015 ) . 
This effort led to the definition of the JATS 1.1d2 schema comprising several tags specifically defined for data citation ( see http : //jats4r.org/data-citations ) . 
Completeness and consistency of data citations can be achieved by providing tools able to automatically create citation snippets for the cited data . 
The creation of citation snippets is often demanded of users ; for instance , describing the research identification initiative , Bandrowski et al . 
( 2015 ) explained that in a federated search context the data creators are asked to insert the correct citation for the data that will be then used as a template provided to the users . 
Users are asked to build the citation snippet manually using the given template as a guide . 
Similar methods are employed by many other relevant scientific databases such as the Eagle‐i open RDF data set ( Torniai , Bourges‐Waldegg , & Hoffmann , 2015 ) , a “ resource discovery ” tool built to facilitate translational science research . 
All the resources in Eagle‐i are citable and , on the one hand , the system provides a “ cite me ” functionality , which returns a text description about how to cite a given resource and where to retrieve all the relevant data ; and on the other hand , the citation snippet has to be composed manually by the user . 
Reactome , a free , open‐source , curated , and peer‐reviewed pathway database for bioinformatics funded by the US NIH ( Croft , 2013 ) uses the same manual method to instruct users about how to cite a specific resource . 
DataVerse assigns metadata to each data set in a static way ; citations to subsets are created starting from the citation of the main data set and by adding some specific information such as UNF , but there is not a dynamic creation of the citation snippet . 
In the nanopublication model ( Groth et al. , 2010 ) , each citable statement is enriched with provenance and attribution annotations that can be used to manually create a citation snippet for the statement . 
It has been highlighted that the manual creation of citation snippets is a barrier to an effective and pervasive data citation practice , as well as a source of inconsistencies and fragmentation in the citations ( White , 1982 ; Thorisson , 2009 ) . 
Indeed , especially for big , complex , and evolving data sets , users may not have the necessary knowledge ( both of the domain and of the technical aspects ) to create complete and consistent snippets . 
Indeed , one of the RDA recommendations for citation systems ( Rauber et al. , 2016 ) regards “ automated citation text generation , ” which is required to lower the barrier for citing and sharing the data . 
The solution sketched out in this context is similar to the one proposed by Bandrowski et al . 
( 2015 ) ; during the ingest phase of a data set , data creators are required to provide descriptive metadata that will be used to create citation snippets . 
Recently , the problem of automatically creating citation snippets has been defined as a computational problem that requires new ideas from computer science to be addressed ( Buneman et al. , 2016 ) . 
Existing techniques to automatically build citation snippets consider a single data model at a time and/or a narrow subset of queries . 
In this context , the XML model has been investigated thoroughly , principally because it is widely used for data sharing and exchange . 
Buneman and Silvello ( 2010 ) proposed a rule‐based system creating citations by using only the information present in the data themselves . 
Given an XML file , this system requires that the nodes corresponding to citable units be identified ( in advance ) and tagged with a rule that is then used to generate a citation ; the form of the rule is where provides a concrete syntax of a human or machine‐readable citation and is a path augmented with some decorated variables . 
The purpose of is to bind the decorated variables in order to use them in . 
Once the given XML file has been prepared to be cited ( i.e. , the rules are in place ) , the citation of a citable unit within this file is generated by a conjunction of the rules retrieved from the node corresponding to the citable unit up to the root of the XML file . 
Basically , the system gathers all the rules in the path from the citable unit to the root and each rule contains a specification of the elements to be comprised in the citation that has to be generated . 
This system allows the automatic generation of both human‐ and machine‐readable citations of single XML nodes . 
Buneman et al . 
( 2016 ) builds on and extends this system by defining a view‐based citation method for hierarchical data . 
The idea is to define logical views over an XML data set , where each view is associated with a citation rule , which if evaluated generates the required citation snippet according to a predefined style . 
This method allows us to create citations for general queries and thus for single resources as well as subsets and aggregations of resources . 
Basically , given a data set D , a query Q , a set of views V defined over D , and a set of citation rules ( one for each view ) , the first step executed by the system is to rewrite Q by using the views in V and thus obtaining a valid rewriting Q′ of Q . 
As an example , we may find that Q′ uses two views , say { V1 , V2 } ; the second step is to take the views used in Q′ and to evaluate the rules associated with them . 
In our example , we would have two rules , one associated with V1 and one with V2 , where each rule generates a valid citation , say C1 and C2 . 
The third step is to employ a predefined function to combine C1 and C2 into a single citation C . 
Davidson , Buneman , Deutsch , Milo , and Silvello ( 2017 ) and Davidson et al . 
( 2017 ) further formalized and extended this work for it to work with general queries for relational databases ; they also highlight the technical relationships between data citation and provenance . 
In the same vein by exploiting database views , Alawini , Chen , Davidson , Portilho Da Silva , and Silvello ( 2017 ) proposed a system for citing single RDF resources . 
They developed a working system tested for the Eagle‐i RDF data set , which creates automatic citation snippets for any RDF graph node , formats them in JSON and in a given human‐readable format , and maintains fixity thanks to an ad‐hoc RDF versioned data store . 
These approaches are well defined and could be implemented to work rather efficiently , but their principal drawback is that the rules as well as the views have to be defined by hand and they require the active involvement of experts ( data creators and data curators ) who know both the data set and specific query languages and data models . 
Silvello ( 2017 ) proposed the learning to cite framework , which builds on the approaches described above , but overcomes their main drawback by automating the creation of rules/views . 
The basic idea is to learn a citation model directly from a given data collection by using a sample set of citation snippets for training purposes and then exploit such a model to build citations on the fly for any citable unit within that collection . 
This system produces citations for single resources ( i.e. , single XML nodes ) which are not formally exact , but as close as possible to what is considered a “ correct citation. ” Silvello ( 2017 ) 's method needs to be revised in order to face the challenging computational problems emerging when working with more general queries and with relational and graph‐based models . 
Guaranteeing the persistency of the data being cited as well as of the citations themselves is a core aspect of data citation . 
The main technique addressing the fixity problem has been put forward by the RDA Working Group on Data Citation ( Rauber et al. , 2016 ) , which proposed a versioning system for relational databases ; this versioning system works in concert with the query store and guarantees that the cited data are always retrievable over time even though the data set has been modified . 
However , in order to address fixity for all the different kinds of scientific data sets , we need to address versioning for all the relevant data models adopted to manage , share , and access scientific data sets ; that is , at least relational databases , XML and RDF data sets . 
Versioning systems must work with time queries because data referred by a citation needs to be retrieved at the time the citation was issued . 
There are some viable solutions for relational databases ( Bohlen , Gamper , Jensen , & Snodgrass , 2009 ) that may require improvements from the efficiency viewpoint and an effective solution for XML ( Buneman , Khanna , Tajima , & Tan , 2004 ) that needs to be extended to work with time queries , whereas RDF‐based versioning requires a major methodological advancement to be usable with time queries ( Geerts , Unger , Karvounarakis , Fundulaki , & Christophides , 2016 ) in the data citation context . 
A first RDF versioning system for RDF data sets is proposed in Alawini et al . 
( 2017 ) even though time queries are not explicitly handled . 
Data infrastructures storing , managing , providing access , and preserving data sets are essential to the development of data citation . 
Only a little more than 5 years ago , De Waard ( 2012 , p. 177 ) pointed out that : “ Overall , commercial publishers are not interested in owning or charging for research data or running repositories . 
