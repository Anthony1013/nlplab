A limitation of the current experimental methodology in information retrieval ( IR ) is that it allows us to evaluate IR systems and search engines only as a kind of “ black‐box , ” without an understanding of how their different components interact with each other and contribute to the overall performances . 
In other terms , the current experimental methodology considers system performances as indivisible and it can not break them down into the contributions of the different components constituting an IR system . 
This severe impediment was pointed out a long time ago by Robertson ( 1981 , p. 12 ) : “ if we want to decide between alternative indexing strategies for example , we must use these strategies as part of a complete information retrieval system , and examine its overall performance ( with each of the alternatives ) directly. ” This limitation has several drawbacks : it prevents us from gaining a thorough understanding of IR system performances ; it precludes the possibility of knowing beforehand which combination of components is best suited for a specific search task or collection of documents ( Fuhr , 2010 , 2012 ) ; and it hampers the possibility of determining in which components it is more convenient to invest effort and resources because they or their combination have the greatest impact in terms of performance gains . 
Therefore , we need to develop some sort of anatomy of IR system performances , meant as a set of methodologies that allow us to study and gain knowledge about the internals of IR system performances and how to combine them . 
In this paper , we start to overcome the limitations of the current experimental methodology and we provide the means to estimate the effects of the different components of an IR system . 
In particular , we develop a methodology , based on the General Linear Mixed Model ( GLMM ) and analysis of variance ( ANOVA ; Rutherford , 2011 ) , which makes use of a grid of points ( GoP ) containing all the possible combinations of inspected components ( Ferro & Silvello , 2016a ) . 
We create extensive GoPs covering six different stop lists , six types of stemmers , eight flavors of n‐grams , and 17 distinct IR models , representing the basic set of common components typically present in any IR system for English retrieval . 
Then the proposed methodology allows us to break down the system performances into the contributions of these stops lists , stemmers , or n‐grams and IR models , as well as to study their interaction . 
The main contributions of the paper are : the methodology for breaking down component effects and analyzing the GoPs across multiple evaluation measures ; the GoPs themselves , a valuable resource that can also be exploited for other kinds of analyses and which is available to the research community.11 http : //gridofpoints.dei.unipd.it/ They also provide a very extensive sample of the most commonly used components for English retrieval ; the application of the proposed methodology to two different search tasks : news search and web search , and their thorough analysis to derive insights on English retrieval . 
Note that the goal of the paper is not to analyze the effects of any possible kind of component , such as word compounding , entity extraction , or query expansion , just to name a few , which you may find in an operational IR system . 
The goal is instead to introduce a methodology that allows us to break down the effects of components and to show an application of this methodology to the basic and common set of components described above , highlighting what it allows us to understand in the context of two relevant search tasks , as news search and web search are . 
Further research could then exploit the proposed methodology and apply it to other kinds of components and search tasks . 
The paper is organized as follows : the Related Works section presents related works ; The Methodology introduces our methodology ; Grid of Points , Measures , and Setup describes the experimental setup and the produced GoPs ; the News Search Analysis and Web Search Analysis sections apply , respectively , the proposed methodology to the analysis of the news and web search tasks ; Power Analysis and Measures Analysis analyzes the component effects and their interaction with evaluation measures ; finally , Conclusions and Future Works draws some conclusions and provides an outlook for future work . 
Experimental evaluation is a core activity in IR but it is very demanding in terms of both the time and effort needed to perform it . 
Therefore , it is usually carried out in publicly open and large‐scale evaluation campaigns at the international level , all based on the Cranfield paradigm ( Cleverdon , 1997 ) , which makes use of shared experimental collections . 
This allows for sharing the effort of producing large experimental collections and comparing state‐of‐the‐art systems and algorithms on a common and reproducible ground . 
The impossibility of testing a single component by setting it aside from the complete IR system is a longstanding and well‐known problem in IR experimentation . 
Component‐based evaluation methodologies have tried to tackle this issue by providing technical solutions for mixing different components without the need of building an entire IR system from scratch . 
However , even if these approaches allowed researchers to focus on the components of their own interest , they have not delivered estimates of the performance figures of each component yet . 
It is well known that topic variability is greater than system variability ( Tague‐Sutcliffe & Blustein , 1994 ) . 
Therefore , a lot of effort has been put into better understanding this source of variance ( Robertson & Kanoulas , 2012 ) as well as making IR systems more robust to it ( e.g. , Zhang , Dawei , Wang , & Hou , 2014 ) , basically by trying to improve on the interaction effect . 
Nevertheless , with respect to an IR system , topic variance is a kind of “ external source ” of variation , which can not be controlled by developers , but can only be taken into account to better deal with it . 
On the other hand , system variance is a kind of “ internal source ” of variation , and because it is originated by the choice of system components , it can be directly affected by developers by working on them , and it represents the intrinsic differences between algorithms . 
The decomposition of performances into system and topic effects has been exploited by Banks , Over , and Zhang ( 1999 ) and Tague‐Sutcliffe and Blustein ( 1994 ) to analyze Text REtrieval Conference ( TREC ) data ; Carterette ( 2012 ) proposed model‐based inference , using linear models and ANOVA , as an approach to multiple comparisons ; Jayasinghe et al . 
( 2015 ) used multivariate linear models to compare nondeterministic IR systems among them and with deterministic ones . 
In all these cases , the goal is a more accurate comparison among systems rather than an analysis and breakdown of system variance per se . 
Robertson and Kanoulas ( 2012 ) applied GLMM to the study of per‐topic variance by using simulated data to generate more replicates for each ( topic , system ) pair to also estimate the topic/system interaction effect . 
Sanderson , Turpin , Zhang , and Scholer ( 2012 ) and Jones , Turpin , Mizzaro , Scholer , and Sanderson ( 2014 ) started a preliminary investigation of the sub‐corpus effect exploiting correlation whereas recently , Ferro and Sanderson ( 2017 ) made it part of a whole model for system performance . 
Overall , to the best of our knowledge , this paper represents the first attempt to apply GLMM and ANOVA to the decomposition of system components effects . 
The idea of creating all the possible combinations of components has been proposed by Ferro and Harman ( 2010 ) , who noted that a systematic series of experiments on standard collections would have created a GoP , where ( ideally ) all the combinations of retrieval methods and components are represented , allowing us to gain more insight into the effectiveness of the different components and their interaction ; this would have also called for the identification of suitable baselines with respect to which all the comparisons have to be made . 
Even though Ferro and Harman ( 2010 ) introduced the idea of a GoP and how it could have been central to the decomposition of system component performances , they did not come up with a fully‐fledged methodology for analyzing such data and breaking down component performances , which is instead the contribution of the present work . 
More recently , the proliferation of open‐source IR systems ( Trotman et al. , 2012 ) has greatly ameliorated the situation , allowing researchers to run systematic experiments more easily . 
Trotman , Puurula , and Burgess ( 2014 ) conducted a vertical exploration of variations of two IR models whereas the “ Open‐Source Information Retrieval Reproducibility Challenge ” ( Lin et al. , 2016 ) provided several reproducible baselines over TREC and Conference and Labs of the Evaluation Forum ( CLEF ) collections . 
Overall , both these efforts added a few points to the ideal GoP mentioned above , but they do not propose any methodology for estimating the component effects . 
We move a step forward with respect to these works because we propose an actual methodology for exploiting such GoPs to decompose system performances and we rely on a much more fine‐grained grid , in terms of the number of components and IR models experimented . 
We aim to decompose the effects of different components on the overall system performance . 
In particular , we are interested in investigating the effects of the components commonly present in almost any IR system : stop lists ; Lexical Unit Generator ( LUG ) , namely , stemmers or n‐grams ; and IR models , such as the vector space , the probabilistic model , or the language models . 
For a detailed survey and description of the main IR system components , refer to Croft , Metzler , and Strohman ( 2009 ) . 
The methodology we adopt relies on the Cranfield paradigm ( Cleverdon , 1997 ) based on experimental collections consisting of a set of documents D , a set of topics T , and the ground‐truth GT which , for each topic , determines the documents relevant for that topic . 
We create a GoP on an experimental collection by running all the IR systems resulting from all the possible combinations of the considered components ( stop list , LUG , IR model ) ; stemmers and n‐grams are considered as mutually exclusive LUG components ; thus , we do not consider IR systems using both stemmer and n‐grams . 
We employ a GLMM to explain the variation of a dependent variable ( “ Data ” ) in terms of a controlled variation of independent variables ( “ Model ” ) in addition to a residual uncontrolled variation ( “ Error ” ) : Data = Model + Error . 
In this context , we are interested both in single independent variables , i.e. , the main effects of the different components alone , and their combinations , that is , the interaction effects between components . 
Note that some independent variables are considered fixed effects —that is , they have precisely defined levels , and inferences about its effect apply only to those levels—which in our case are different kinds of systems and components ; and some others are considered random effects —that is , they describe a randomly and independently drawn set of levels that represent variation in a clearly defined wider population—which in our case are the topics . 
In our experimental design , systems and their components are the experimental conditions ( factors ) , and topics are the subjects . 
Given that each topic is processed by each system , we have a repeated measure design , which has the advantages of reducing the error variance due to the greater similarity of the scores provided by the same subjects and increasing the statistical power for a fixed number of subjects . 
The adopted design is the one typically used when ANOVA is applied to the analysis of the system performances in a track of an evaluation campaign , as in Banks et al . 
( 1999 ) and Tague‐Sutcliffe and Blustein ( 1994 ) , where the subjects are the topics and the factors are the system runs . 
Basically , in this context ANOVA is used to determine which experimental condition dependent variable score means differ , that is , which systems are significantly different from others . 
On top of this , and unlike the state‐of‐the‐art , we are also interested in determining which proportion of variation is due to the topics and which one to the systems . 
Furthermore , we also aim to determine the proportion of variance explained by each component of a system . 
We define a three‐factors design where we manipulate factors A , B , and C corresponding to the stop lists , the LUG , and the IR models , respectively ; with this design we can also study the interaction between component pairs as well as the third‐order interaction between them . 
In this design the systems are decomposed into three main constituents : ( a ) factor A ( stop lists ) with p levels where , for instance , A 1 corresponds to the absence of a stop list , A 2 to the indri stop list , A 3 to the terrier stop list , and so on ; ( b ) factor B ( LUG ) with q levels where B 1 corresponds to the absence of a LUG , B 2 to the Porter stemmer , B 3 to the Krovetz stemmer , and so on ; ( c ) factor C ( IR models ) with r levels where C 1 corresponds to BM25 , C 2 to TF*IDF , and so on . 
We define a four‐factor design that extends the three factors one defined above . 
The first three factors remain the same—that is , stop list , lug , and model ; whereas the fourth factor D is represented by the different evaluation measures . 
Each available evaluation measure represents a different angle to weight and understand system performances . 
The goal of this new model is twofold : ( a ) to further dig into the components contribution without taking into account the influence of the adopted evaluation measure ; and ( b ) to understand how measures interact with the different components . 
Different evaluation measures are not directly comparable with each other , as you would expect from the factorial design above , because they embed different user models . 
Therefore , even though all the measure scores are in the range is not the same as because they exploit the scale in different ways . 
In order to smooth these differences and make the scores more directly comparable , we normalized them by the maximum value achieved on the dataset and then reasoned in terms of ratios . 
For this analysis , we also consider the interaction effects among these factors , for example , is the interaction between an IR model and a measure . 
We do not consider third‐ and fourth‐order interactions such as reporting the interaction between stop lists , LUGs , and IR models , because they are rarely significant . 
Finally , is the error committed by the model in predicting the score of the i‐th subject in the four factors . 
The common rule of thumb ( Rutherford , 2011 ) when classifying effect size is : 0.14 and above is a large effect , 0.06–0.14 is a medium effect , and 0.01–0.06 is a small effect . 
values could happen to be negative and in such cases they are considered as zero . 
When conducting experiments , two types of error may happen . 
A Type I error occurs when a true null hypothesis is rejected and the significance level α is the probability of committing a Type I error . 
We keep Type I errors controlled by applying the Tukey Honestly Significant Difference ( HSD ) test with a significance level . 
Tukey 's method is used in ANOVA to create confidence intervals for all pairwise differences between a given factor level means while controlling the family error rate . 
A Type II error occurs when a false null hypothesis is accepted and it is concerned with the capability of the conducted experiment to actually detect the effect under examination . 
Type II errors are often overlooked because if they occur , although a real effect is missed , no misdirection occurs and further experimentation is very likely to reveal the effect . 
We considered three main components of an IR system : stop list , LUG , and IR model . 
We selected a set of alternative implementations of each component and , by using the Terrier22 http : //www.terrier.org/ open‐source system ( Macdonald , McCreadie , Santos , & Ounis , 2012 ) , we created a run for each system defined by combining the available components in all possible ways . 
The components we selected are : Stop list : nostop , indri , lucene , snowball , smart , terrier ; LUG : nolug , weak Porter , Porter , snowball Porter , Krovetz , Lovins , 4grams , 5grams , 6grams , 7grams , 8grams , 9grams , 10grams ; Model : BB2 , BM25 , DFIZ , DFRee , DirichletLM , DLH , DPH , HiemstraLM , IFB2 , InB2 , InL2 , InexpB2 , Js_KLs , LemurTFIDF , LGD , PL2 , TFIDF . 
The stoplists differ from each other by the number of terms composing them ; specifically , indri has 418 terms , lucene has 33 terms , snowball has 174 terms , smart has 571 terms , and terrier 733 terms . 
As for the LUG component , we consider two distinct classes : stemmers and n‐grams . 
Stemmers can be classified into aggressive and weak stemmers . 
One of the first stemmers developed for IR systems is Lovins ; this is an iterative affix removal stemmer that removes the longest possible string of characters from a word , according to a set of rules . 
Lovins is the most aggressive stemmer among those we consider . 
The Porter algorithm and its variants ( snowball and a weaker version ) is inspired by the Lovins algorithm , but it adds machine‐readable dictionaries and well‐defined rules for morphology . 
The Krovetz algorithm adds a word‐disambiguation algorithm to the Porter stemmer . 
n‐grams are LUG components alternative to the stemmer that can be defined as a contiguous sequence of n items from a given sequence of text . 
Character n‐grams are a language‐independent alternative to complex language‐specific tokenization . 
We consider seven different n‐gram lengths ranging from n = 4 to n = 10 ; we did not employ 3‐grams or smaller lengths because they are very ineffective for the English language and the runs employing this component would be treated as outliers in the experiments . 
The models we employ are classified into the three main approaches currently adopted by search engines : ( a ) the vector space model : TFIDF and LemurTFIDF ; ( b ) the probabilistic model—including the BM25 models and the Divergence From Randomness ( DFR ) models , and in particular : BB2 ( Bose‐Einstein model ) , DFIZ , DFRee ( hyper‐geometric model ) , DLH ( parameter free DFR model ) , DPH ( DFR model with Popper 's normalization ) , IFB2 ( ITF with Bernoulli 's processes ) , InB2 ( a variant of the PL2 ) , InL2 ( IDF model for randomness with Laplace succession ) , InexpB2 and PL2 ( Poisson estimation using Laplace succession ) ; and ( c ) the language models , and in particular : DirichletLM ( LM with Bayesian smoothing and a Dirichlet Prior ) , HiemstraLM ( Hiemstra 's LM ) , Js_KLs ( Jefrreys ’ divergence with Kullback Leibler 's divergence ) , and LGD . 
We consider the stemmers and n‐grams and mutually exclusive LUG components because they represent very different approaches to lexical unit generation and , typically , without prejudice to the other components , they lead to alternative pipelines . 
Thus , we end up with two distinct groups of runs , one using the stemmers and one using the n‐grams ; the nolug component is common to both these groups instantiated as nostemmer for the stemmer group and as nograms for the n‐grams one . 
The stemmer group defines a factorial design with a GoP consisting of 612 runs ; the n‐grams group defines a factorial design with a GoP consisting of 816 runs . 
Note that the above GoPs consider whole components and do not further break them down into subcomponents . 
For example , IR models are not broken down into their constituents , as studied by Zobel and Moffat ( 1998 ) . 
In addition , for IR models , we use the default setup of their parameters , as provided by Terrier , and we do not explore here variations of this setup , which would lead to much bigger GoPs ; this is left for future work.33 To ease reproducibility , the code for running the experiments is available at : http : //gridofpoints.dei.unipd.it/ . 
We used the following standard and shared collections : TREC 07 Adhoc track , TREC 08 Adhoc track , TREC 09 web track , and TREC 10 web track . 
TREC 07 and TREC 08 focus on a news search task and adopt a corpus of about 528K news documents ( i.e. , disks 4 and 5 of the TIPSTER collection minus the Congressional Record ) ; both TREC 07 and 08 provide 50 different topics and a set of binary relevance judgments—that is , given a topic , a document is judged either not relevant or relevant . 
TREC 09 and TREC 10 focus on a web search task and adopt a corpus of 1.7M web pages ( i.e. , the WT10g collection ) ; both TREC 09 and 10 are composed of 50 different topics and a set of graded relevance judgments—that is , not relevant , relevant , and highly relevant . 
We conducted the experiments on two collections composed of 100 topics each , one created by combining the topics of TREC 07 with those of TREC 08 and the topics of TREC 09 with those of TREC 10 . 
We evaluated the GoPs by employing nine different evaluation measures : AP , P @ 10 , Rprec , RBP , nDCG , nDCG @ 20 , ERR , ERR @ 20 , and Twist . 
Average precision ( AP ) represents the “ gold standard ” measure in IR , known to be stable and informative , with a natural top‐heavy bias and an underlying theoretical basis as approximation of the area under the precision/recall curve . 
Precision at Ten ( P @ 10 ) is the classic precision measure with cutoff at the first 10 retrieved documents . 
This measure is defined for binary relevance judgments and is particularly well suited for evaluating web tasks . 
RPrec is precision calculated with cutoff at the recall base—that is , the total number of relevant documents for a given topic determined by the ground truth . 
Rank‐biased precision ( RBP ; Moffat & Zobel , 2008 ) is built around a user model based on the utility a user can achieve by using a system : the higher , the better . 
The model it implements is that a user always starts from the first document in the list and then s/he progresses from one document to the next with a probability p . 
We calculated RBP by setting p = .8 , which represents a good trade‐off between a very persistent and a remitting user . 
These measures are based on binary relevant judgments and thus can be naturally applied to TREC 07 and TREC 08 . 
For TREC 09 and TREC 10 , we performed a lenient mapping of the relevance judgments by considering as relevant both highly relevant and relevant documents . 
Normalized Discounted Cumulated Gain ( nDCG ; Järvelin & Kekäläinen , 2002 ) is the normalized version of the widely known DCG , which discounts the gain provided by each relevant retrieved document proportionally to the rank at which it is retrieved . 
nDCG is defined for graded relevance judgments and it is one of the most common measures used for evaluating web search tasks . 
For TREC 07 and 08 , we calculated nDCG in a binary relevance setting by giving gain 0 to nonrelevant documents and gain 5 to the relevant ones , whereas for TREC 09 and TREC 10 we assign a weight 0 to nonrelevant documents , 5 to the relevant ones , and 10 to the highly relevant ones . 
Furthermore , we used a log 10 discounting function . 
nDCG is calculated up to the last relevant retrieved document , whereas nDCG @ 20 is calculated up to rank position 20 . 
Expected Reciprocal Rank ( ERR ; Chapelle , Metzler , Zhang , & Grinspan , 2009 ) is a measure defined for graded relevance judgments and for evaluating navigational intent . 
It is particularly top‐heavy because it highly penalizes systems placing not‐relevant documents in high positions . 
For TREC 07 and TREC 08 , we calculated ERR in a binary relevance setting as we have done for nDCG . 
ERR is calculated up to the last relevant retrieved document , whereas ERR @ 20 is calculated with cutoff at 20 , thus focusing on the top retrieved documents . 
Twist ( Ferro , Silvello , Keskustalo , Pirkola , & Järvelin , 2016 ) is a measure for informational intents , which handles both binary and graded relevance . 
Twist adopts a user model where the user scans the ranked list from top to bottom until s/he stops , and returns an estimate of the effort required by the user to traverse the ranked list . 
Twist evaluates systems from the viewpoint of the avoidable effort for their users by accounting for their fatigue while visiting a nonideal ranking of documents ; thus , it evaluates IR systems from a different angle , i.e. , user effort than other measures such as nDCG and ERR , which are more focused on user 's gain . 
We use the three‐way GLMM of the Analysis of Component Effects section to carry out the analysis of IR system components . 
In the following , we focus on AP as reference measure ; the analysis across measures is reported in the Power Analysis and Measures Analysis section . 
Table 1 reports the estimated SOA for all the main and interaction effects and , within parentheses , the p‐values for all the ANOVA three‐way tests we conducted ; this table presents the results divided by search task ( news search in the upper part or web search in the lower part ) and , within a search task , by LUG group ( stemmers or n‐grams ) . 
For the stemmers group , we can see that stop lists are a medium‐size effect followed by IR models and stemmers , which are both small‐size effects . 
All second‐ and third‐order interactions between components are not significant with the notable exception of the Stop lists * IR models interaction , which is a medium‐size effect and the biggest among all the effects . 
Overall , this suggests that stops lists are a key component for the stemmers case and that they even influence the IR models . 
From the main effects plot of the stop lists in the upper part of Figure 1 , we can see that there is a substantial difference between systems employing or not a stop list while the impact of different stop lists is less marked , even though the corresponding Tukey HSD plot shows that the lucene stop list , that is , the shortest one , is not in the top group , thus marking a significant loss in performances with respect to the others . 
TREC 07‐08 news search : main effects , interaction effects , and Tukey HSD plots for average precision on stemmers and n‐grams groups . 
[ Color figure can be viewed at wileyonlinelibrary.com ] When it comes to stemmers , the main effects and the Tukey HSD plots show that the best performances are obtained by the krovetz stemmer , followed by the Porter‐based stemmers ; systems employing the lovins stemmer or not employing a stemmer at all report significantly lower performances , even if lovins is still significantly better than no stemming . 
Finally , as far as IR models are concerned , the top group is composed , in descending order , by inexpb2 , inb2 , and inl2 , which are three variations of the DFR probabilistic model ; it is also possible to identify a second group including the tfidf ( a vector space model ) , jskls ( a second generation DFR model ) , and lgd ( a bridge between language and DFR models ) . 
It is interesting to note that , for news search , the effect of bm25 alone is not in the first two top‐performing groups , despite the fact that it is one of the widest employed models in IR and often the default choice when employing off‐the‐shelf search engines . 
The interaction plot in the upper right of Figure 1 shows the joint effects of stop lists and IR models : we can see that there is some interaction , in general , due to the fact that lines tend to cross but , mostly , we can see that there are IR models that significantly suffer from the lack of a stop list , namely , ifb2 , bb2 , pl2 , bm25 , and dfiz . 
This means that a generally well‐performing model , as bm25 is , changes performances a lot with or without a stop list—for example , mean average precision ( MAP ) 0.2381 with lucene stop list and krovetz stemmer versus MAP 0.1575 without stop list and without krovetz stemmer ; it also implies that the same model may perform worse than a generally lower‐performing model , but less influenced by stop lists , as in the case of dirichletlm—MAP 0.1967 with lucene stop list and krovetz stemmer and MAP 0.1910 without stop list and without krovetz stemmer . 
In particular , the small interaction between stop lists and language models has been argued by Zhai and Lafferty ( 2004 , p. 48 ) : “ the effects of stop word removal should be better achieved by exploiting language modeling techniques. ” The interaction plot in Figure 1 supports this claim because the effect of the stop list on hiemstralm is almost null and on dirichletlm is very small . 
