At its broadest level , privacy is a protection that people enact to selectively control others ' access to themselves ( Altman , 1975 ; Westin , 1967 ) . 
In the real world privacy management involves the control of interpersonal boundaries to reduce discrepancies between one 's desired and actual levels of privacy ( Altman , 1975 ) . 
To control this boundary , people take cognitive ownership of their private information and consider information to be private when individuals believe that the information belongs to them ( Petronio , 2002 ) . 
In the real world , private information is often implicitly protected by mutually understood , unwritten , and normative cognitive rules of sharing ( e.g. , anything personal you tell your friend is expected to be kept private ) and through explicit legal guarantees ( e.g. , anything personal you reveal to your therapist is legally protected ) . 
In social media , however , these rules have to be actively managed by the individual through the use of the privacy management settings provided by platforms such as Facebook . 
The privacy management settings Facebook affords can be broadly categorized using the classification proposed by DeCew ( 1997 ) into information privacy , accessibility privacy , and expressive privacy . 
These categories , although built prior to the advent Web 2.0 , where the boundaries between private and public information is increasingly blurred , provide a convenient framework for dilenating the different privavy settings Facebook affords . 
Information privacy focuses on the control of access to one 's digital information . 
This includes user‐disclosed information on Facebook , such as name , gender , residence , birth date , employment and education history , contact information and relationship status , which collectively constitute one 's social identity . 
Social identity is further linked to one 's private and economic life and a breach of this data could lead to serious issues such as identity theft and online harassment . 
As discused , a hacker with access to these pieces of digital information could easily steal an user 's identity , open credit cards , or procure access to other online accounts . 
The protection measure available to control access to this information on Facebook can be found it Security Settings ” ( Figure 1 ) , which monitors login information and notifies users of suspicious activity . 
Screen shot of Facebook 's security settings . 
[ Color figure can be viewed at wileyonlinelibrary.com ] Accessibility privacy involves control over the acquisition of information that could provide access to the individual ( DeCew , 1997 ) . 
Accessibility privacy focuses on the access to individuals rather than their private information ( i.e. , information privacy ) and is premised on the idea that , at their core , individuals do not wish to be found or disturbed by others . 
This presumption corresponds to Westin 's ( 1967 ) classical privacy theory that emphasizes solitude , intimacy , anonymity , and reserve . 
On Facebook , accessibility privacy involves managing how one is found on the platform . 
To be found by your friends and acquaintances is , on one hand , a necessary step toward realizing any of the rewards from social media , but on the other hand , being found makes users ' easier targets of phishers and scammers who can utilize these accessible pieces of personal information to craft attacks . 
For instance , merely being searchable on social media makes it possible to target the user with a phony friend‐request , a common type of social network‐based phishing attack where the attacker impersonates another person and attempts to friend the user or sends a malware‐laden message through the platform . 
To protect one 's anonymity , Facebook 's Privacy Settings and Tools ( Figure 2 ) help users control who can find them using email or phone numbers , as well as who can send them private messages and friend‐requests . 
Screen shot of Facebook 's privacy settings and tools . 
[ Color figure can be viewed at wileyonlinelibrary.com ] Expressive privacy involves control over how one expresses one 's self‐identity or personhood through speech or activity ( DeCew , 1997 ) . 
Self‐disclosure on social media is , in some ways , similar to public performances , as described by Goffman ( 1959 ) , where individuals control or guide the formation of impressions by altering one 's appearance or manner . 
Like these on‐stage performances , Facebook users maintain desired impressions based on different times , audiences , or situations . 
However , the audiences on Facebook are large , diverse , and reflect overlapping social spheres ( Joinson , Houghton , Vasalou , & Marder , 2011 ) . 
Consequently , norms and expectations differ across these social spheres , necessitating a tailoring of online sharing to accommodate these disparate audiences . 
Users can manage their social image by selective self‐presentation , or by controlling other‐provided information that could affect one 's social image ( e.g. , photo tags and comments from a user 's friends ) . 
Undesired audiences and a loss of control over one 's social image could result in damaged reputations ( Debatin , Lovejoy , Horn , & Hughes , 2009 ) . 
The control of one 's social image on Facebook is enacted through the Timeline and Tagging Settings ( Figure 3 ) , which manages who can add to a user 's timeline , see posts on a timeline , or add tags . 
Screen shot of Facebook 's timeline and tagging settings . 
[ Color figure can be viewed at wileyonlinelibrary.com ] The enactment of these three sets of Facebook privacy settings are based on a cognitive assessment of the costs of ostensibly getting hacked , breached , or maligned against the many benefits that could accrue from being available , accessible , and reciprocally active on social media . 
The likely costs that drive the enactment of Facebook 's privacy protections are examined next using PMT . 
PMT was first introduced by Rogers ( 1975 ) to explore the effects of fear appeal messages on persuasion . 
This framework describes four factors that are cognitively processed when a threat is presented : perceived severity , perceived susceptibility , response efficacy , and self‐efficacy ( Rogers , 1975 ) . 
Perceived severity and perceived susceptibility represent threat appraisal , whereas response efficacy and self‐efficacy represent coping appraisal ( Rogers , 1983 ) . 
These appraisals involve personal considerations of the breadth and scope of the threat and the individuals capability of being able to deal with it . 
Thus , together , they signify the costs associated with a perceived risk or threat . 
PMT predicts that people are most likely to adopt recommended protective behaviors when they believe that the associated costs from it is likely to be high , that is , the threat is serious , that they are susceptible to it , and that they can execute these behaviors on their own and these enacted behaviors can prevent the risk ( Rogers , 1983 ) . 
The theory has been successfully applied to explain the enactment of a wide range of personal health‐related behaviors ( Floyd , Prentice‐Dunn , & Rogers , 2000 ) , including the prevention of adolescent drug trafficking ( Wu , Stanton , Li , Galbraith , & Cole , 2005 ) , improving security policy compliance in organizations ( Herath & Rao , 2009 ) , and even antinuclear war behaviors ( Wolf , Gregory , & Stephan , 1986 ) . 
More recent research has extended it to the technology‐related risk domain to study online privacy intrusion ( Meso , Yi Ding , & Shuting Xu , 2013 ; Milne , Labrecque , & Cromer , 2009 ; Sangmi , Bagchi‐Sen , Morrell , Rao , & Upadhyaya , 2009 ) , identity theft ( Lai , Li , & Hsieh , 2012 ) , online spying and hacking ( Chenoweth , Minch , & Gattiker , 2009 ) and other , broad I.T . 
threats ( Liang & Xue , 2009 ; Moser , Bruppacher , & Mosler , 2011 ) . 
Almost all these studies suggest an enhancement of protections in the face of increased cognitive cost and coping appraisals . 
Although none of this work focuses on the enactment of privacy protections on social media , they altogether provide the impetus for hypothesizing that increased cost and coping appraisals of risks from social media use would lead to an enhancement of the protections afforded on Facebook to control information , accessibility , and expressive privacy . 
This leads to the following hypotheses about the relationship between PMT based threats and the privacy protections afforded by Facebook : H1 : User 's ( a ) perceived severity , ( b ) perceived susceptibility , ( c ) response efficacy , and ( d ) self‐efficacy toward unauthorized access of digital data will enhance the frequency and use of Facebook 's privacy protections to control information privacy .H2 : User 's ( a ) perceived severity , ( b ) perceived susceptibility , ( c ) response efficacy , and ( d ) self‐efficacy toward loss of anonymity will enhance the frequency and use of Facebook 's privacy protections to control accessibility privacy.H3 : User 's ( a ) perceived severity , ( b ) perceived susceptibility , ( c ) response efficacy , and ( d ) self‐efficacy toward the potential loss of social image will enhance the frequency and use of Facebook 's privacy protections to control expressive privacy . 
H1 : User 's ( a ) perceived severity , ( b ) perceived susceptibility , ( c ) response efficacy , and ( d ) self‐efficacy toward unauthorized access of digital data will enhance the frequency and use of Facebook 's privacy protections to control information privacy . 
H2 : User 's ( a ) perceived severity , ( b ) perceived susceptibility , ( c ) response efficacy , and ( d ) self‐efficacy toward loss of anonymity will enhance the frequency and use of Facebook 's privacy protections to control accessibility privacy . 
H3 : User 's ( a ) perceived severity , ( b ) perceived susceptibility , ( c ) response efficacy , and ( d ) self‐efficacy toward the potential loss of social image will enhance the frequency and use of Facebook 's privacy protections to control expressive privacy . 
Although PMT‐based appraisals are expected to enhance protections , considerations of the perceived benefits from social media are expected to stymie the enactment of protections . 
The perceived benefits of Facebook are examined next using the lens provided by the Uses and Gratifications paradigm . 
The U & G paradigm is an audience‐centric approach that presumes people have innate needs or benefits that they seek to satisfy through the selection and use of media . 
Recent applications of this paradigm have led to the discovery of a variety of gratifications driving Facebook use depending on whether the research focused on top‐level gratifications or specific Facebook functionalities . 
For instance , Ray ( 2007 ) found three first‐order drivers of Facebook use , primarily information , surveillance , and entertainment use . 
With a focus on specific functionalities , Raacke and Bonds‐Raacke ( 2008 ) found that people use Facebook for keeping in touch with old and new friends , locating old friends and making new friends , as well as posting and looking at photos . 
In line with the latter approach , Joinson ( 2008 ) found seven unique U & G of Facebook including social connection , shared identities , content , social investigation , social network surfing , and status updating . 
The present research focused on the first‐order gratifications people derive from Facebook rather then the gratifications they derive from its individual functions . 
This is because Facebook 's offerings are constantly evolving with technology , limiting the implications of functionality‐focused research findings to the short‐term . 
A guide to the top‐level gratifications from Facebook comes from a recent exhaustive review of U & G studies from 1940 to 2011 that found media use to be driven chiefly by considerations of social , identity , information , and entertainment needs ( Sundar & Limperos , 2013 ) . 
These broadly map on to the three top‐level gratifications made possible through Facebook use . 
Specially , the gratifications are : information ( e.g. , users can follow current events and various organizations ' updates ) , social ( e.g. , make friends and keep in touch with old friends ) , and entertainment ( e.g. , play games ) . 
Fulfilling information needs require seeking knowledge about events and people ; social needs involve being abreast with the happenings around us in the social world ; and entertainment needs require mood alleviating activities . 
Satisfying these through Facebook require a degree of openness , availability , and accessibility by other users on the platform . 
For instance , Facebook users ' information needs can be fulfilled by subscribing to public Facebook pages ' timelines , which makes a user 's profile visible to the page admin and all others who follow the same page . 
Social needs can be fulfilled by sharing personal thoughts and activities through status updates . 
Finally , fulfilling entertainment needs on Facebook can be done by playing social games or shopping , which requires interaction through the site , often with strangers , that can not be accomplished without revealing profile information . 
Thus , realizing the benefits of Facebook require the relaxation of the afforded privacy controls , which lead to the following hypotheses : H4 : The perceived ( a ) social needs , ( b ) information needs , and ( c ) entertainment needs fulfilled by Facebook will reduce the frequency and use of Facebook 's privacy protections to control information privacy.H5 : The perceived ( a ) social needs , ( b ) information needs , and ( c ) entertainment needs fulfilled by Facebook will reduce the frequency and use of Facebook 's privacy protections to control accessibility privacy.H6 : The perceived ( a ) social needs , ( b ) information needs , and ( c ) entertainment needs fulfilled by Facebook will reduce the frequency and use of Facebook 's privacy protections to control expressive privacy . 
H4 : The perceived ( a ) social needs , ( b ) information needs , and ( c ) entertainment needs fulfilled by Facebook will reduce the frequency and use of Facebook 's privacy protections to control information privacy . 
H5 : The perceived ( a ) social needs , ( b ) information needs , and ( c ) entertainment needs fulfilled by Facebook will reduce the frequency and use of Facebook 's privacy protections to control accessibility privacy . 
H6 : The perceived ( a ) social needs , ( b ) information needs , and ( c ) entertainment needs fulfilled by Facebook will reduce the frequency and use of Facebook 's privacy protections to control expressive privacy . 
Data for the study were gathered using a cross sectional survey of undergraduate students enrolled in communication classes at a large Northeastern university . 
A total of 513 participants ( 57 % male ) responded to an IRB approved online survey over a two‐week period in the fall of the 2014 . 
DeCew 's ( 1997 ) dimensions of privacy can be operationalized using the three different types of setting Facebook affords for privay management : Information privacy can be measured based on how users manage their Facebook Security Settings ; Accessiblity privacy can be measured based on how users manager their Facebook Privacy Settings and Tools ; and Expressive privacy can be measured based on how users manage their Facebook Timeline and Tagging Settings . 
The Facebook interface affords users the ability to individually manage subsetting within each of these settings ( as shown in Figures 1-3 ) . 
To measure these , individual items were developed that measured the extent to which participants were aware of the setting or their level of openeness of the setting ( using a 1–5 scale , not at all aware–very aware or open to none–open to everyone ) followed by the frequency with which they changed that setting ( not at all frequent/infrequent–very frequently ) . 
Using these two evaluation dimensions netted a two‐dimensional semantic differential measure that captured potency and intensity , which were then multipled ( i.e. , Potency × Intensity ) to derive a weighted score . 
This resulted in four broad groups of users : those who were minimally aware of a setting and used it infrequently ; the minimally aware frequent users ; the maximally aware infrequent users ; and the maximally aware frequent users . 
The overall scores ranged from 1–25 , with scores being tied for individuals who had low awareness and high frequency ( score of 5 ) and those who have high awareness and low frequency ( score of 5 ) because the net effect of the two behaviours are thought to be more or less the same . 
That is people who do n't know what they are doing and do it often might mistakenly use the wrong settings , which is the same as someone knowingly not changing their settings.11 This premised on the idea that keeping settings unchanged leaves the user vulnerable because technology is constantly evalolving as are cyber attacks . 
This is precisely why settings on platforms change frequently and users are often instructed to review settings and change things like their passwords frequently . 
Hence , leaving things set in any one way—the proverbial “ build a single big wall around a house ” —does not assure protection and does not reflect how privacy settings are managed by users . 
Six items measured participants ' level of awareness of the protective measures that Facebook offers to prevent the loss of digital data within the Security Settings category on the interface . 
Sample items include “ I am aware of the Secure Browsing setting ” and “ I am aware of the Login Notifications setting. ” Exploratory Factor Analysis ( EFA ) was conducted on the items , mean = 3.13 , SD = 0.90 , α = 0.88 . 
The items were weighted by the respective frequency of change response and averaged to create an index for final modeling , mean = 6.17 , SD = 3.90 . 
Four items measured participants ' level of openness toward allowing other users to contact them . 
Sample items drawn from Facebook 's Privacy Settings read : “ Who can look you up through Facebook using your email address or phone number ” and “ Who can send you private messages. ” Similarly , an EFA was conducted on the items , mean = 2.73 , SD = 0.75 , α = 0.71 . 
The items were weighted by the respective frequency of change response and averaged to create an index for final analysis , mean = 5.42 , SD = 3.48 . 
Six items measured participants ' level of openness toward allowing other users to see their timeline or to add to it . 
Sample items read : “ Who can see the status updates or photos you post ” and “ Who do you accept status updates or photo tagging requests from. ” An EFA was conducted on the items , mean = 3.38 , SD = 0.78 , α = 0.92 . 
Those items were then weighted by the respective frequency of change response and averaged to create an index for final analysis , mean = 6.42 , SD = 3.37 . 
The independent variable of PMT was measured using a Likert‐type scale of 1 ( Strongly disagree ) to 5 ( Strongly agree ) . 
Nine items measured participants ' perceived level of severity toward losses from each dimension of online privacy . 
Sample items read : “ Having my personal information or photos stolen through Facebook would be a serious problem for me ” ( unauthorized access of digital data ) , “ Having people I do not know send me messages or friend requests through Facebook would be a serious problem for me ” ( loss of anonymity ) , and “ Having people I do not know see my status updates or photos in Facebook would be a serious problem for me ” ( loss of social image ) . 
EFA was conducted on the items and two dimensions were revealed . 
The first dimension was related to unauthorized access of digital data , mean = 3.61 , SD = 0.87 , α = 0.85 , and items in this dimension were averaged to create an index for testing H1 and H4 . 
The second dimension was related to loss of anonymity and social image , mean = 3.14 , SD = 0.78 , α = 0.82 , and the responses in this dimension were averaged to create an index for testing H2 , H3 , H5 , and H6 . 
Nine items measured participants ' level of susceptibility from attacks targeted at each dimension of online privacy . 
Sample items read : “ I feel that I could be subjected to identity theft or hacking through Facebook ” ( unauthorized access of digital data ) , “ I feel that people I do not know can easily send me messages or friend requests through Facebook ” ( loss of anonymity ) , and “ I feel that people I do not know can easily see my status updates or photos in Facebook ” ( loss of social image ) . 
Again , an EFA netted two dimensions and a closer inspection of items suggested that one dimension was related to unauthorized access of digital data , mean = 3.21 , SD = 0.68 , α =0.83 . 
Responses on this dimension were averaged into an index for testing H1 and H4 . 
The other dimension was related to loss of anonymity and social image , mean = 2.91 , SD = 0.88 , α = 0.87 , and the responses in this dimension were averaged to create an index for H2 , H3 , H5 , and H6 . 
Five items measure participants ' beliefs about the extent to which Facebook 's privacy settings were effective in protecting their account . 
Sample items read : “ By changing my privacy settings , I believe that Facebook will protect my personal information or photos ” and “ If someone is trying to access my personal info , Facebook privacy settings are effective to prevent it. ” An EFA was conducted on the items before the responses were averaged to create an index , mean = 3.20 , SD = 0.75 , α = 0.89 . 
Seven items measured participants ' beliefs that they could manage their Facebook privacy settings on their own . 
Sample items read : “ I believe that I have the ability to protect my personal information on Facebook ” and “ I feel confident that I can change my Facebook privacy settings without help from anyone. ” An EFA was conducted on the items before the responses were averaged to create an index , mean = 3.56 , SD = 0.73 , α = 0.91 . 
The independent variables of U & G were measured using a Likert‐type scale of 1 ( Strongly disagree ) to 5 ( Strongly agree ) . 
Based on prior research ( Joinson , 2008 ; Raacke & Bonds‐Raacke , 2008 ; Ray , 2007 ; Sundar & Limperos , 2013 ) , 17 items measured participants ' likely benefits from Facebook . 
Example items read : “ I use Facebook because I can make new friends ” and “ I use Facebook because it diverts my attention when I am bored or stressed. ” An EFA conducted on the items revealed three dimensions mapping on to the three gratification dimensions explicated by prior research : social needs , mean = 2.83 , SD = 0.79 , α = 0.87 , information needs , mean = 3.35 , SD = 0.77 , α = 0.81 , and entertainment needs , mean = 3.43 , SD = 0.80 , α = 0.80 . 
The responses in each dimension were averaged to create an index . 
H1 and H4 , H2 and H5 , and H3 and H6 , examined the relative influence of PMT based costs versus U & G based benefits , on the enactment of information privacy , accessibility privacy , and expressive privacy protections afforded by Facebook , respectively . 
Each set of hypotheses was tested using a hierarchical regression with PMT‐based perceived costs and U & G‐based benefits as independent measures , and the weighted variable measuring enactment of the respective Facebook 's privacy setting as the dependent variable . 
In each model , the PMT factors indicating perceived costs were entered first followed by the U & G based benfits . 
The rationale for this stems from a large body of cognitive research in contexts ranging from advertising to message framing that shows the salience of fear appeals and perceived losses over benefits during decision‐making ( Vishwanath , 2009 ) . 
Thus , perceived severity and perceived susceptibility toward unauthorized access of digital data , the user 's response efficacy , and self‐efficacy were entered in the first block , followed by the U & G variables ( i.e. , social needs , information needs , and entertainment needs ) in the second block . 
The first model testing H1 and H4 was significant , F ( 7 , 505 ) = 15.06 , p < 0.001 , explaining 16 % of the variance in the enactment of Facebook 's information privacy settings . 
Table 1 presents the tests . 
Perceived susceptibility , β = .13 , t = 3.07 , p = .002 , positively predicted protection enactment toward digital data . 
In addition , social needs positively predicted protection enactment , β =.43 , t = 8.53 , p < .001 , whereas information needs , β =‐.14 , t = −2.55 , p = .011 , negatively predicted protection enactment toward unauthorized access of digital data . 
Therefore , H1 and H4 were partially supported . 
First block ( PMT ) Second block ( U & G ) Next , The analysis was repeated to examine the enactment of accessibility privacy settings afforded by Facebook . 
Table 2 presents the results from testing H2 and H5 . 
First block ( PMT ) Second block ( U & G ) The regression model was also significant , F ( 7 , 505 ) = 16.70 , p < .001 , and explained 18 % of the variance in the protection enactment . 
In the model , perceived severity , β = .19 , t = 4.51 , p < .001 , positively predicted protection enactment , while self‐efficacy negatively predicted protection enactment , β = −.11 , t = −2.20 , p = .029 . 
Thus , H2 was partially supported . 
Among the U & G predictors , social needs positively predicted protection enactment , β = .38 , t = 7.47 , p < .001 ; protection enactment was negatively predicted by information needs , β = −.14 , t = −2.52 , p = .012 , and entertainment needs , β = −.12 , t = −2.33 , p = .02 ) . 
Based on the findings , H5 was partially supported . 
Lastly , the model testing H3 and H6 concerning the enactment of expressive privacy protections afforded by Facebook was also significant , F ( 7 , 505 ) = 13.84 , p < .001 , and explained 15 % of the variance in privacy enactment . 
The results of these tests are presented in Table 3 . 
In this model , perceived severity , β = .24 , t = 5.49 , p < .001 , positively predicted protection enactment . 
From the U & G constructs , social needs , β = .31 , t = 6.002 , p < .001 , positively predicted protection . 
The research examined the cognitive cost‐benefit appraisal processes that underlie users ' management of various privacy protections on Facebook . 
DeCew 's ( 1997 ) dimensions of privacy provided a mechanism for categorizing the afforded protections . 
