Moreover , there is a greater interaction between the stop list and the lgd model , probably because this is a hybrid model that bridges between language models and the DFR models , which typically exhibit a sizable interaction with stop lists . 
Overall , we observe a kind of increasing interaction between stop lists and the family of probabilistic models : almost no interaction for the language models ; some interaction for the DFR models ; and , a lot of interaction for the classic bm25 model . 
Finally , the vector space model , namely , tfidf and lemurtfidf , shows almost no interaction with the stop lists . 
The interaction between stemmers and IR models reported in Table 1 is almost null and not significant but , as we will discuss in below , we are lacking the statistical power to be able to detect it . 
Nevertheless , if we consider the interaction plot for stemmers * IR models shown in Figure 1 , with the previous caveat about the size and significance of the effect , we notice that all the IR models are somehow affected by the use of a stemmer and some of them—bm25 , hiemstralm , inb2 , inl2 , inexpb2 , lemurtfidf—seems to be affected more by the lack of a stemmer . 
Overall , this supports the importance of the stop lists noted above and it is quite a striking result because , nowadays , it is commonly thought that stop lists were more useful in the past to reduce the index size and skip useless calculations for efficiency reasons , whereas their impact on effectiveness should have been limited and basically hidden by IR models because they assign almost zero weight to stop words . 
For the n‐grams group , we can see from Table 1 that all the effects , with the exception of third‐order interactions , become significant even if they are all now small‐size effects , compared to the small and medium size they were in the case of the stemmers group . 
We can observe that the effect of IR models becomes more pronounced than the effect of the stop lists , whereas n‐grams have less of an impact than stemmers , about half of the effect size . 
Moreover , the interaction Stop lists * IR models is as big as the IR models effect alone , and this suggests that stop lists play a central role also in the case of n‐grams . 
The interaction between stop lists and n‐grams and IR models and n‐grams , even though significant , is almost negligible in terms of size . 
From the lower part of Figure 1 , as far as the main effects of stop lists are concerned , we observe the same behavior as in the case of stemmers : there is a marked difference between using or not a stop list and lucene stop list is the only one outside the top group . 
Regarding the main effects of n‐grams , they are not effective for news search because the top‐performing systems are those not employing n‐grams or those employing the 10‐grams , which is a very mild form of character sequencing . 
In particular , we can see that the 5‐grams component is statistically as effective as the 9‐grams component—that is , they are the second‐best performing group ; from 6‐ to 8‐grams we can see a progressive degradation of performance . 
For the IR models effect , the top group consists of inexpb2 and inb2 , whereas the second top group consists of lemurtfidf and inl2 ; therefore , in comparison to the stemmers group , inexpb2 and inb2 confirm themselves as the top‐performing approaches , whereas in the second group we always find a vector space model ( either tfidf or lemurtfidf ) . 
Regarding the interaction between stop lists and IR models , we observe also in the n‐grams case a behavior similar to that of the stemmers group : ifb2 , bb2 , pl2 , bm25 , and dfiz models greatly suffer from the lack of a stop list . 
The interaction plot of n‐grams with IR models shows marked interaction . 
Moreover , even though systems employing nolug perform in general better than those employing no form of character sequencing , some models benefit from employing a 4‐ or 5‐grams component , as with bm25 , inexpb2 , and lemurtfidf where the use of 5‐grams increases performance with respect to not using them . 
Overall , we can appreciate that n‐grams interact with IR models more than stemmers , even though they are less effective than stemmers for English retrieval , as also noticed by Büttcher , Clarke , and Cormack , ( 2010 ) . 
Moreover , stop lists play a central role both alone and in conjunction with IR models and their importance is basically independent of the LUG component adopted . 
Table 1 shows the web search task , which displays a trend similar to the case of the news search tasks , even though it is more pronounced , with some notable differences we will discuss . 
For the stemmer group , the stop lists and IR models yield medium‐size effects while the stemmers show a small‐size effect , almost negligible in this case ; regarding interaction effects , the Stop lists * IR models yields the biggest effect , which is of medium size , while all the others are not significant . 
From the main effects and Tukey HSD plots of the stop lists in Figure 2 , we can see that , again , there is a substantial difference between systems employing or not a stop list , while the top‐performing stop list is terrier , that is , the longest one , followed by a second group constituted by snowball , smart , and indri ; lucene is still the lowest‐performing stop list . 
The fact that there is a clearer distinction between stop lists than in the news search case , and that the longest stop list is the top‐performing one , led us to hypothesize that the noisy web context benefits more from the aggressive filtering of a longer stop list . 
TREC 09‐10 web search : main effects , interaction effects , and Tukey HSD plots for average precision on stemmers and n‐grams groups . 
[ Color figure can be viewed at wileyonlinelibrary.com ] As far as stemmers are concerned , the Porter‐based stemmers constitute the top group in the case of web search , while krovetz and lovins stay together in the second group , well above the group employing no stemmer at all . 
With respect to the news search case , the less aggressive stemmers perform better for web search and this may be motivated again by the hypothesis that the noisy web context benefits more from avoiding further noise due to overstemming . 
Regarding IR models , the top group is constituted by dph , a DFR probabilistic model , and jskls , a second‐generation DFR model requiring no parameter tuning , whereas the second group includes dfree , inexpb2 , inl2 , and tfidf ; therefore , we observe a kind of swap between the first and the second group with respect to the news search case . 
The interaction plot in the upper right of Figure 2 shows the joint effects of stop lists and IR models : it shows even more interaction with respect to the news search case and the number of IR models that severely suffer from a lack of a stop list increases—now ifb2 , bb2 , pl2 , dfiz , dlh , and bm25—even though in this case bm25 is much less impacted from the lack of a stop list . 
Moreover , language models exhibit a higher interaction with stop lists for the web search task than for news search , in particular for dirichletlm ; by contrast , both the vector space models , i.e. , tfidf and lemurtfidf , confirm a low interaction with the stop lists as observed for the news search task . 
Regarding the interaction between stemmers and IR models , bearing in mind the caveats pointed out in the previous section , in the web search scenario a number of models seem to be affected by the lack of a stemmer , namely , bm25 , inb2 , inl2 , inexpb2 , jskls , lemurtfidf , lgd , pl2 , and tfidf . 
Moreover , in this case , the difference in interaction between not using and using a stemmer is higher than the difference between the use of different stemmers . 
For the n‐grams group , all the effects , apart from third‐order interactions , are significant and the IR models yield the biggest effect , a medium‐size one , followed by n‐grams , Stop lists * IR models interaction , and Stop lists , which are all small‐size effects . 
Regarding the news search task , we can observe the more pronounced role of the IR Models and n‐grams effects , while the impact of stop lists is a bit reduced , with the interaction Stop lists * IR models still being the second‐biggest effect . 
From the lower part of Figure 2 , we can note again the impact of using a stop list also in the case of n‐grams with terrier , which is the top‐performing stop list , followed by all the others now in the same second group . 
This supports the hypothesis of the benefits of a longer stop list in the noisy web context , made even noisier by the application of n‐grams . 
The n‐grams component always performs significantly worse than no n‐grams at all and it seems to be more harmful for web search than for news search . 
Regarding IR models , the top group contains lemurtfidf , a vector space model , and inexpb2 , a DFR probabilistic model , which were , respectively , in the second and the first top group in the case of news search . 
If we look at the interaction between stop lists and IR models , we can observe the same list of models suffering from the lack of a stop list , although bm25 is less impacted than in the previous case . 
The interaction plot between n‐grams and IR models shows that there is no model benefiting from n‐grams , unlike the news search case , where some models benefited from using a 4‐ or 5‐grams component . 
Table 1 also reports the estimated SOA for all the other considered measures ; this emphasizes the difference between the various measures when it comes to breaking down component effects . 
In general , all the measures agree on which effects are not significant , for both the stemmers and the n‐grams groups and both the news and web search tasks , with the only exception being P @ 10 , where the Stemmers * IR models interaction is significant for the stemmers group in the news search tasks , even though its effect size is almost negligible . 
It is interesting to note that the effect sizes with nDCG and Twist are bigger than with the other measures ; in particular , nDCG and Twist greatly enhance the impact of stop lists , IR models , and their interaction , as well as putting more emphasis on LUG , both stemmers and n‐grams . 
On the other hand , ERR and ERR @ 20 show the opposite behavior by reducing the effect size of these components with respect to AP ; indeed , for these measures the effect size of stop lists is small and not medium , as it is with all the other measures , and the impact of LUG , both stemmers and n‐grams , becomes almost negligible . 
This suggests that not all the measures are equally good at detecting differences between the components and they have quite a diverse perception of what the internals of an IR system are . 
To further investigate this issue , we compute the statistical power using the G*Power tool44 http : //www.gpower.hhu.de/ : Table 2 reports the statistical power of the main effects and the most interesting interaction effects . 
We can observe that , in general , we have good or high power in most of the cases , which ensures we are able to actually detect the effects we are looking for . 
In particular , the best and most robust measures in terms of statistical power are nDCG , Twist , and AP because they exhibit enough power in almost all the circumstances ; on the other hand , the lower‐performing measures are ERR and ERR @ 20 , closely followed by other top‐heavy measures such as P @ 10 , nDCG @ 20 , and RBP . 
More in detail , we can note how ERR and ERR @ 20 do not have enough power to detect the LUG effects , both stemmers and n‐grams on both news and web search tasks , and how they also lack some power for detecting stop lists effects when n‐grams are employed . 
This confirms that ERR and its variants are not robust measures because they require more topics than other measures to detect reliable effect sizes ( Sakai , 2016 ) . 
In general , stemmer effects are the most difficult to detect for most measures , the web search task being more challenging than news search . 
It is generally thought that stemming is not particularly important for English retrieval , especially with respect to other European languages , but these analyses show that we may actually be in the situation of not properly detecting its effects due to the lack of power . 
Because p‐values and significance are affected by the sample size and we would need to increase it to obtain enough power , it may be also worth reconsidering the Stemmers * IR models interaction , which might not be properly detected in the current setting . 
To further investigate the impact of measures , we employ the four‐way GLMM of the Analysis of Components and Measure Effects section to carry out the analysis of IR system components by considering the measures as a factor contributing to explain the system variance . 
In Table 3 we report the estimated SOA for all the main and interaction effects , the p‐values for all the ANOVA four‐way tests we conducted , and the statistical power . 
This table presents the results divided by experimental collection and , within a collection , by LUG group . 
The most noticeable phenomenon is the huge proportion of variance explained by the measures whose effect sizes are up to three orders of magnitude bigger than those of the IR components and that the interaction between components and measures is always significant even if we have a small size effect . 
Furthermore , this analysis allows us to point out the contribution of IR components and their interactions , without considering the influence of the evaluation measures . 
In this respect , the main trends observed in the previous sections are supported : for the news search task , stop lists yield the biggest effect , followed by IR models and stemmers or n‐grams ; and , for the web search task , IR models yield the biggest effect followed by n‐grams , stop lists , and stemmers . 
It is interesting to note that , independent of the measure effects , almost all the second‐order interactions are now significant even though of small/negligible size . 
Regarding the power , we observe a loss of power with respect to the three‐way analysis of the News Search Analysis and Web Search Analysis sections . 
This gives us a feeling about how much the variability between measures in detecting component effects actually impacts our analyses and the conclusions we draw . 
We addressed a previously unsolved issue in the IR evaluation methodology and proposed an innovative solution to break down the performances of an IR system into the effects of its components to understand their contributions to the overall performance as well as their interactions . 
To this end , we developed an analysis methodology consisting of two elements : a GoP created on standard experimental collections , where all the combinations of system components under examination are considered ; and a GLMM to decompose the contribution of these components to the overall system variance , paired with some graphical tools to easily assess the main and interaction effects . 
Note that such a controlled experimental setting is typically not available in evaluation campaigns , such as TREC , where participating systems do not constitute a systematic sampling of all the possible combinations of components and often are not even described in such detail to know exactly what components have been used . 
We conducted a thorough experimentation on TREC collections , considering two different tasks—news search and web search—and adopting several evaluation measures . 
For each task , we created GoPs consisting of nearly all the state‐of‐the‐art stop lists , stemmers , and n‐grams , and IR models available for English retrieval . 
We found that the most prominent effects are those of stop lists and IR models , as well as their interactions , whereas stemmers and n‐grams play a smaller role . 
As a general observation , we found that linguistic resources play a prominent role in English retrieval and that their interaction with IR models can have such a big influence to completely change the model performance . 
We also highlighted the lack of some statistical power to properly detect the effects of stemmers and their interaction with IR models . 
This might be a consequence of the common knowledge that stemming does not have much impact on English retrieval . 
Nevertheless , these conclusions should be further validated with a sample size big enough to reliably detect these effects . 
Finally , we have seen that measures explain a large portion of system variance and that different measures detect system and component effects differently . 
We conclude that not all the measures are suitable for all the cases , as for ERR and ERR @ 20 , which almost fail to detect the stemmer effect . 
On the other hand , nDCG , Twist , and AP proved themselves to be stable and robust , being able to detect components effect size with good or even high statistical power . 
As far as future work is concerned , we plan to extend the proposed methodology to also be able to capture the interaction between topics/systems and topics/components . 
Indeed , to estimate interaction effects , more replicates would be needed for each ( topic , system ) pair , as Robertson and Kanoulas ( 2012 ) simulated , and they are not possible in the present setting because running the same system on the same topics more than once produces exactly the same results . 
Furthermore , we will investigate how to apply the proposed methodology in the context of the analysis of MultiLingual Information Access ( MLIA ) components . 
We have already started to prepare GoPs in many European languages using CLEF collections ( Ferro & Silvello , 2016b ) . 
To study them , we need to extend the current methodology to allow for comparisons across languages . 
Indeed , unlike the case handled in this study , within each component family , we are actually experimenting with components that differ from language to language , e.g. , an aggressive French stemmer is intrinsically different from a German one . 
As a consequence , this setting may require a GLMM which makes use of random effects instead of the fixed ones currently used for representing the various components . 
