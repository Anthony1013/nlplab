Of these , 149 were Australian and 78 Swedish . 
Scholars from both countries referred to the examples of use ( CVs , grant , and promotion applications ) included in the survey , some providing context around that use . 
For instance , an Australian used Google Scholar citations “ to embed in a narrative for grant applications , ” and another had used the h‐index , “ heavily qualified , ” but “ necessary given the scientific lean of the promotion panel. ” Additional purposes of use were noted , the most frequent being annual ( performance ) reviews and reporting to the institution . 
Citations and the impact factor were used by an Australian to show “ I 'm hitting institutional targets , ” and another Australian had used metrics “ to satisfy the uni that I am following their directions and requirements. ” Many social scientists described their use of metrics with terms that attribute a value and we identified several recurrent terms—quality , impact , and quantify . 
Quality was associated with all the main metrics presented in Figure 3 and generally related to publications , for example “ to highlight the quality of publications ” and “ h‐index which is best correlated with quality in the current publication climate. ” One participant had used the ERA journal list for a promotion application and wrote “ shorthand for quality even though it isn't. ” Impact was associated with the scholars ' research and was employed in a less descriptive manner , such as “ to demonstrate impact ” and “ to indicate the impact of my work. ” However , the comments of a number of scholars hinted at specific uses , such as “ to demonstrate international impact , ” and altmetrics had been used to demonstrate “ wider impact. ” When using the term quantify it was in a similarly limited form as impact , although a Swedish scholar noted using the impact factor and h‐index to “ quantify the spread ” of their work . 
Scholars also used metrics to benchmark or compare with others in their field , to indicate reputation and esteem , to provide evidence of scholarly activity , “ capacity for institutional mobility , ” and to “ sell ” their work . 
Evidence of a rather more directive environment can be seen in the comments of two Swedish scholars ; one stating they were “ forced to use an h‐index ” and the other had used citations “ because counting them was required. ” In the same vein , an Australian wrote “ we are mandated to use ” Google Scholar , Scopus , and the Journal Citation Reports . 
Another Australian indirectly referred to their institution as an important driver in the use of citation‐based indicators , with : “ to ward off any criticisms that … articles are not in A* or A ranked journals. ” Mentions of specific alternative metrics were few ; instead , the social media platforms that generate metrics were referred to . 
An Australian scholar 's comment demonstrates a high level of awareness of different sources of metrics , listing ResearchGate and Academia “ stats because they can be better than Google Scholar and Scopus for my discipline. ” Scholars commented on metrics , publishing trends , and expectations of their institution or broader research environment in the open questions . 
In some instances , it was difficult to disentangle the different aspects of academic lives to identify influencing factors in the use of metrics ; however , two interconnected matters emerged strongly : an increased focus on journal publications and research policy at institutional and national levels . 
The emphasis on journal publications was associated with ranked journal lists and indexing by the main citation databases , which often equates to English language , international journals . 
Australian participants were particularly candid about ranked journal lists , while their Swedish peers commented more frequently on international journals and the indexing coverage of journals , although a few referred to the Norwegian and the ABS journal lists . 
A change in publication practices was evident across all social science fields , even Law , for which “ journal metrics were not important a decade ago ” and “ bibliometrics and rankings is [ sic ] getting increasingly important. ” In general , scholars noted a “ decline in the importance of books , ” and in Sweden a shift “ from monographs in Swedish to articles in international journals ” and “ increased importance on publishing in English. ” On this point , only three of the Swedish scholars who reported using metrics indicated that their preferred language of publication was Swedish . 
Early career researchers ( ECRs ) in both countries remarked on the pressure to publish with specific metrics in mind , illustrated by : “ in the past , just having journal articles as an ECR was important , now it appears that the journals need to be well known ” and “ I am trying to combat the shift towards international publication by also writing in Swedish , but it is hard when you are at an early stage in your career : you need those Norwegian listed journal publications. ” As the findings in the previous section indicate , metrics are used in CVs , promotion , and grant applications and this use could be a matter of individual choice . 
However , there was evidence that grant agency requirements , at least in some cases , were a factor and the research environment was heavily criticized for the “ neurotic attention to ranking indicators in recent years ” and “ an obsession with quantifiable research outputs. ” In Australia , this was expressed most often in association with the continued use of the ERA ranked journal list by institutions to reward ( or penalize ) scholars for their research outputs , articulated in the following comment : The insidious ranking of academic journals in Australia … the Faculty uses the repudiated ARC ranking of academic journals to dispense financial rewards for research and conference attendance , and to reduce teaching time . 
The whole matter creates gross perverse incentives . 
The insidious ranking of academic journals in Australia … the Faculty uses the repudiated ARC ranking of academic journals to dispense financial rewards for research and conference attendance , and to reduce teaching time . 
The whole matter creates gross perverse incentives . 
Many social scientists wrote of their institutions ' policies and performance measures relating to preferred research outputs ( journal articles ) and the “ value ” and “ relevance ” attributed to different publication types . 
For example , Australian scholars wrote : “ journal articles have increased in value in terms of promotion ” and “ my uni no longer sees books or book chapters as relevant or even counts them among outputs. ” An awareness of the limitation of metrics being used for performance measurement was also noted : “ The University appears obsessed with citation indexes , and metrics of all sorts , as proxies for ‘ quality ’ research ’ and ‘ I now aim for journals formerly ranked as ‘ A ' under the officially defunct ERA journal ranking system . 
This is what my university uses to evaluate performance in the absence of any other useful measure. ” Swedish scholars wrote of similar institutional policies , again with a focus on journal articles as the preferred publication . 
Their comments included : “ institution policy is to reward publication in journals that are listed in Web of Science or at least on the Norwegian list ” ; “ more and more international , peer‐reviewed articles due to policy for allocation of state research funding to web‐of‐science [ sic ] articles ” ; and “ nowadays much more focus on indexing in Scopus/WoS due to bibliometric evaluation of performance . 
It has been widely acknowledged that the scholarly communication practices of many social science fields are largely incompatible with applying metrics . 
Yet a substantial number of social scientists in our study , particularly in Australia , have used these tools . 
Our data do not provide an explanation of why Swedish social scientists are less inclined to use metrics ; however , we suggest two possibilities . 
First , the difference between how each country has implemented their national research evaluation model is notable . 
In Australia , the creation of the ranked journal list to assess all article outputs in the ERA has attracted a great deal of discussion and debate ( Bennett , Genoni , & Haddow , 2011 ; Knott , 2015 ) , which , as this study shows , remains a contentious issue today . 
On the other hand , although the Swedish national research evaluation model uses citations data to assess all disciplines , the model is not fully integrated into all institution 's research performance systems and therefore there is less expectation that scholars use metrics for other purposes . 
A second , and perhaps a more fruitful line of thought , is that language of publication is a factor . 
Swedish participants noted a shift to more English language ( journal ) publishing , which lends weight to an argument that due to the coverage of the major citation databases , metrics were largely unavailable in the past and therefore not habitually used . 
The emphasis on English language evident in the Swedish responses suggests , however , that their use of metrics is likely to increase in the future . 
When considering metrics use at the field level , we acknowledge that the low number of responses limits the authority of our findings . 
Nevertheless , they support previous work relating to citation coverage and publication practices for Psychology , which used metrics ( citations and h‐index ) in the highest proportions , and Law , which used citation‐based tools ( h‐index and impact factor ) least ( Butler , 2008 ; Verleysen & Weeren , 2016 ) . 
It seems likely that Law 's much higher use of ranked journal lists is the flip‐side of the low metrics use for the field . 
The research also confirms one of the few previous studies of citations use in the social sciences , with Sociology scholars using all the citation‐based tools in relatively high numbers ( 45 % ) ( Buela‐Casal & Zych , 2012 ) . 
Terms that pervade the research evaluation vocabulary ( quality , impact , and quantify ) appeared frequently in the scholars ' comments , and the context in which they were applied suggest pragmatism and compliance in relation to metrics use . 
Metrics were used in promotions and grants applications to match the scholars ' perceptions of assessors ' preferences and to demonstrate research achievements more widely . 
Performance reviews and meeting institutional targets were mentioned , and at times scholars ' responses were expressed as a defense of their work . 
Some chose to use a particular tool because it provided a “ better ” metric for the field , while others sought to contend the prevailing policy by presenting alternative measures to highlight the value of their work . 
These actions and reactions by social scientists suggest a high level of awareness of some metrics and strategic behavior in demonstrating research performance . 
A cause‐and‐effect cycle may exist between scholars ' use of metrics and the emphasis on journal publications by institutions . 
As Naylor wrote in 2001 ( in relation to the Research Assessment Exercise ) , the journal article is “ king , ” and these publications are most able to provide quantifiable assessments in an environment that is focused on a “ results‐oriented vocabulary ” ( Dahler‐Larsen , 2014 , p. 981 ) . 
In this environment , scholars need to be aware of the tools that will identify “ quality ” journals and that may also mean using other metrics to support performance reviews , grant , and promotion applications . 
Burrows wondered if metrics were “ experienced as oppressively ” as he had suggested in his article ( 2012 , p. 369 ) . 
The frank comments about institutional policies in relation to what was valued as a research output , together with the metrics relevant to those outputs , indicate that for ~15 % of social scientists in this study the answer is yes . 
These scholars appear to be experiencing significant tensions in their academic life , evident in the Law participants ' remarks about changes in their field and also in observations about the shift away from books to journal articles . 
As previous research has shown ( Laudel & Gläser , 2006 ; Wouters , 2014 ) , an ambivalent attitude towards using metrics is not uncommon , and Laudel and Gläser ( p. 293 ) describe the tension emanating from “ a message from the exogenous evaluation that directly contradicts the endogenous valuation of communication channels by their disciplines. ” In this study , fields that traditionally produce books are seeing the value of those research outputs diminishing , which may in turn lead to the breaking of “ existing habits and convictions ” ( Dahler‐Larsen , 2012 , p. 14 ) . 
Tensions between intradisciplinary criteria of quality and external evaluation systems ' measures were also found when studying humanities scholars and their attitudes towards metrics ( authors , in press ) . 
Other tensions can be seen for young researchers , who appeared vexed by some expectations for academic achievement . 
This finding evokes Malsch and Tessier 's ( 2015 , p. 85 ) autoethnographic research about ranked journal lists in accounting , in which the term “ identity fragmentation ” is used to describe junior researchers ' experience of being driven “ professionally and intellectually in contradictory directions. ” Our study 's findings are based on a very small proportion of social scientists working in Australia and Sweden , resulting in low numbers of participants in the eight fields . 
This affects the degree to which we can make strong claims about the use and attitudes to metrics in the social sciences . 
Furthermore , the study 's topic may have attracted scholars who felt strongly about research evaluation , which would explain the many forthright comments provided . 
In relation to the social science field , the OECD classification created an element of ambiguity as to the research area with which participants identified . 
If a similar study was to be conducted , using free text for the field question , may produce more precise results in order to compare differences across the social sciences . 
Many social scientists use metrics , despite the literature and expert opinion that warns against it in most fields . 
However , there appears to be an awareness , at least for some , of the limitations of these tools in ascribing quality . 
While Australian social scientists use metrics in much higher proportions than their Swedish peers , the valuing of journal articles in international publications in Sweden is likely to result in much higher use of metrics in the future . 
The importance of institutional policies regarding incentives and disincentives in performance management and academic promotion came through clearly in the data . 
In both countries there is an increasing focus on journal articles and it appeared to be associated with indicators such as the ERA ranked journal list and citation‐based indicators . 
This was accompanied by ambivalence towards metrics , with some scholars expressing resistance , disbelief , or frustration with the research evaluation policies that are in place . 
Both institutional policies and those relating to research evaluation at a national level were mentioned and in many cases it is not possible to separate one from the other due to the expected trickle‐down effect within a national higher education system . 
While a number of scholars referred to specific metrics that are components of their respective national research evaluation system , the strong focus on institutional issues suggests several interconnected factors are associated with the use of metrics by social scientists in Australia and Sweden . 
Finally , it is useful to consider how nations and institutions can alleviate the less positive experiences that appear to be associated with research evaluation and the metrics used to assess research . 
Institutions are likely to adopt aspects of a national model and they are also driven by factors such as university rankings . 
However , a balance needs to be struck between effective research policy and recognition of the nonmetric contribution made by the social sciences . 
Adjustments are possible , and the introduction of engagement and impact assessment in Australia , in 2018 will provide social scientists with alternative methods to demonstrate the value of their work . 
Sweden has also mooted the use of “ societal impact ” to assess research , which in turn may create more opportunities for publications in the national language . 
