The most influential national policy stream developed through TRIPs was the 1998 US Digital Millennium Copyright Act ( DMCA , 1998 ) that emphasized Digital Rights Management ( DRM ) . 
Four new paragraphs were introduced into title 17 , Chapter 12 of the US Code ( US Congress , 1998 ) . 
The most important issue was how to deal with the entity “ digital copy ” ( Gillespie , 2007 ) . 
Two main ideas were introduced that would be of tremendous importance for the development of future digital practices . 
First , in paragraph 1201 : “ Circumvention of copyright protection systems , ” removal of any kind of copyright protection system was prohibited . 
This set the stage for DRM . 
Second , there was an important shift in the rights of the user from ownership to leasing . 
This involved an emphasis on the end‐user license agreement , which allowed a market shift from buying to license‐to‐use . 
License‐to‐use could , for instance , involve the right to play music legally acquired on a CD player but not on a computer . 
As a consequence , music freely downloaded from the file sharing platform Napster , started in 1999 , appeared to be a superior product , as MP3‐files did not contain any DRM protection system and could be played on any device ( Smith , 2003 ; Sterne , 2006 ) . 
Given the ease of Internet‐based file sharing , the enforcement of DRM required development of systematic surveillance technologies of digital watermarking and registration of who does what ( Bygrave , 2002 ) . 
However , such practices also raised concerns relating to privacy and data protection . 
At an ambitious Policy Colloquium co‐convened by Google and O'Reilly Media ( Hemerly , 2012 ) , it was suggested that harvesting data from the private sector , the public sector , and from research required separate policy perspectives . 
Notions of licensing data were probed in order to create a stronger sense of ownership compared to that given by contractual obligations . 
However , the consensus seemed to be that an extended scope of licenses would , in today 's policy context , be difficult to establish . 
Similar problems were connected to the notion of data commons , as people often are not good at anticipating usages of the data they make available . 
The copyright management system of YouTube , Content ID , was taken as an example of a technology that balanced the rights of the user with the appropriate data access needed for innovation . 
Hemerly ( 2013 , p. 28 ) has argued that in some cases “ regulation that intends to protect individuals constrains the use ” and that , therefore , regulators need to create structures for voluntary informed consent participation . 
Essentially , the strong ownership viewpoint of the rights holder has allowed the development of numerous data practices . 
Still , conventional contractual agreements seem to be inefficient for numerous usages . 
At the same time , it is prudent to take note of the observation by Sell and May ( 2001 ) that the balance between strong and weak ownership historically has shifted on a regular basis . 
Given the revelations by Edward Snowden , starting in 2012 ( Greenwald , 2014 ) , it is quite possible that policy is beginning to move toward another shift . 
The new European General Data Protection Regulation ( GDPR ) in 2016 ( European Parliament and Council , 2016 ) introduces specific digital rights for European citizens , including stronger opt‐out rights , the right to be forgotten , and rights to access data stored about them . 
This regulation also broadens and makes more distinct what is meant by personal data , clearly specifying e‐mail addresses , websites , medical information IP addresses , cookies , genetic data , biometric data , fingerprint data , retinal scan data , and location data . 
In addition , the traditional view of what is considered sensitive personal data has also been broadened . 
The traditional list included racial or ethnic origin , political opinions , trade union membership , religious or philosophical beliefs , health data , and sexual orientation . 
GDPR adds to this list genetic data and biometric data . 
This is a radical change with fundamental repercussions for data work . 
The end‐user license agreement was used as a basis for a contractual model , regulating data exchange , involving two other key documents : terms of service and privacy policy . 
These documents emphasized the traditional problem of a content rights holder and customer . 
Typically , users of social media platforms such as Facebook or Google+ were seen as content rights holders of the documents , music , videos , and so forth , produced . 
More important , behavioral data , which lacked regulatory shielding , was collected , processed , packaged , and sold in various ways ( Gehl , 2013 ) . 
Collection , processing , and packaging of data was in this way developed without a firm legal framework defending the rights of users . 
In the absence of legal privacy requirements , established practices of deidentification , including anonymization , key‐coding , encryption , and so forth , were established . 
In addition , Privacy‐Enhancing Technologies ( PET ) , in which users control and manage their individual digital identities , were developed ( Hansen et al. , 2004 ) . 
Nonetheless , critics have argued that such practices or tools are insufficient in supplying credible protection in the face of existing and evolving tools for reidentification ( Ohm , 2010 ) . 
The development of privacy by contract and the surveillance of DRM access became connected to the business model of Web 2.0 , as suggested by O'Reilly ( 2007 ) . 
This built on the strategy of generating massive user‐generated content on free‐for‐use proprietary platforms and thereafter packaging and selling behavioral data . 
Following this , social media platform owners have explored potential forms of commodification in partnership with an evolving business sector of data brokers . 
These rapidly developing practices have flourished despite a rather fragile legal context focused on IPR , that is , user‐generated content , rather than user‐generated behavior . 
Through contractual agreements , social media users now routinely accept that platform owners either exploit behavioral data or the data being produced or both . 
Given that the most powerful corporate actors are US‐based ( such as Google , Amazon , Facebook , Apple , and Microsoft ) it is American legal requirements that have served as a norm for the development of regulation in most countries . 
The most important restriction placed on platform owners in the US , and therefore for the rest of the world , is the necessity of allowing users the opportunity to “ opt out , ” that is , rejecting the routinized transfer of ownership and selling of personal data . 
However , Turow ( 2012 ) found that available functions for generating individual consent in the form of opting out often were difficult to find , navigate , or simply not working as advertised . 
Tene and Polonetsky ( 2013 ) considered the development of a practice of “ opt in ” rather than “ opt out ” as a possible way forward . 
Another approach , building on the agency of users , has been to suggest “ privacy by design , ” which involves the building of databases without any personal data ( Hustinx , 2010 ) . 
This strategy is closely connected to PET , as well as to the principle of data minimization , which involves a minimized amount of personal information stored and shared as well as duration of time stored . 
The emergence of metadata capital on the part of providers ( Greenberg , Ogletree , Murillo , Caruso , & Huang , 2014 ) , along with practices of participatory personal data on the part of users ( Shilton , 2012 ) , continue to make this aspect of platform use a matter of live debate . 
Notably , many of these ideas , including overarching principles of privacy by design , have been integrated into the European GDPR . 
Not surprisingly , given this context of increasing transparency and behavioral tracking , a new field of surveillance studies has evolved . 
In one of the early and formative works of this interdisciplinary field , Andrejevic ( 2009 ) introduced the notion of a continuously expanding digital enclosure , in which heavily annotated information on everything people around the world did in their daily lives could be cross‐referenced and searched from all angles . 
Andrejevic noted that more and more “ analog ” activities were brought into the digital enclosure each year , continuously minimizing the amounts of public and private spaces that were not monitored , recorded , and processed . 
Solove ( 2004 , 2007 , 2011 ) developed a sophisticated critique of the development of privacy in the context of increasing transparency of personal data . 
Most notable was his deconstruction of the “ I have nothing to hide ” argument , which has been the common gut reaction from users when routinely waving what was earlier seen as fundamental rights ( Schneier , 2006 ) . 
Solove ( 2011 ) identified weaker ( I have nothing to hide so others , not me , should be scrutinized ) and stronger ( I 'm willing to sacrifice privacy for national security ) variations of this argument . 
The digital enclosure available for Google increased dramatically as , in 2012 , it created a common privacy policy for some 50 services . 
This enabled Google to view data streams from sources such as YouTube , Google search engine , Google apps , Google Earth , and Google analytics as one single enclosure of data . 
Google has argued that this allows users increased privacy control functionality ( http : //privacy.google.com/ ) . 
However , user tools available are limited compared to what has been suggested by proponents of privacy‐enhancing identity management technology ( Hansen et al. , 2004 ) . 
In addition , van Dijck ( 2013 ) points out that it is deceptive to imply to users that they can control sharing of personal information , as controls only apply to the front‐end ( network of friends ) and not to the back‐end ( third parties who purchase data ) . 
A central issue involved in ethical discussions is whether PII can be successfully anonymized . 
This is basically a technological discussion surrounding the ubiquitous possibility of reidentification of once anonymized data ( Ohm , 2010 ) . 
Confronted with this problem , Google employee Hemerly ( 2013 , p. 30 ) suggests injection of “ additional noise ” in order to make reidentification more difficult . 
From that perspective , the value of data‐driven innovation is so substantial that privacy concerns should be taken into account but not be allowed to block progress . 
Hansen , Schwartz , and Cooper ( 2008 ) suggest several features to strengthen privacy‐oriented identity management . 
These include minimal disclosure tokens ( utilizing cryptographic software to create multiple private certificates of identity ) , machine‐readable privacy policies ( forcing corporations to uphold privacy policies beyond the bare baseline for informed consent ) , sticky policies ( tagging data with relevant privacy policy to avoid misuse by third parties ) , and transparency tools ( historical data track tools that allow users to see what information has been disclosed to whom ) . 
It should also be borne in mind that privacy is not only a matter of personal values and preferences , but also may involve a contextual element where the trade‐off between the risks of disclosure and the benefits of a service can be moderated , for example by usage experience and trust ( Martin & Shilton , 2015 ) . 
Vitak , Shilton , and Ashktorab ( 2016 ) further address how ready access to online data is presenting researchers with new ethical challenges of respect , beneficence , and justice . 
The previous section highlighted the structural conditions at a macro‐level pertaining to two key issues involved in negotiating a trade‐off between the value and risks of data work at a micro‐level . 
The following section highlights how organizational initiatives in information and data governance have attempted to address the balance between the value and risks of data work at a meso‐level.11 Attention is restricted here to organizational issues . 
It is important to bear in mind , however , that similar issues of governance and accountability will be worked out in the extra‐organizational context of consortia and the like ( see , for example , Cutcher‐Gershenfeld et al . 
( 2017 ) . 
The term “ governance ” has Latin and Greek roots that associate it with the idea of “ steering. ” The “ idea of steersman—the person at the helm—is a particularly helpful insight into the reality of governance ” ( Tricker , 1984 , p. 9 ) . 
Governance can be further defined as “ the fact that ( a person , etc . 
) governs ; the action or manner of governing ; and the state of being governed ” ( Onions , 1973 , p. 874 ) . 
This definition further expands the concept and means that an adequate understanding of governance includes at least the following concerns : Who is governing ? 
How is governance accomplished and via which mechanisms , for instance , legal and other regulations , policy‐making , professional guidelines , decision‐making procedures ? 
To what is governance being directed , for instance , data , information , documents , and electronic records . 
Therefore , a full appreciation of the emergence of data as an informational resource , along with negotiating the status of this resource as a source of value and risk , requires attention not only to the macro‐context but also to the organizational conditions at a meso‐level impinging on data work at a micro‐level ( Foster , 2016 ) . 
Understandings of information governance have largely emerged out of the literatures relating to corporate governance , IT governance , information systems , and , latterly , information studies , while understandings of data governance have emerged more from business computing and computing science . 
The literatures also have different emphases . 
Whereas information governance focuses squarely on the trade‐off between value and risk , data governance tends to look more to accountability issues , structural responsibilities , and decision‐making capacities , in response to external regulatory pressures as well as organizational goals . 
Smallwood ( 2014 , p. 6 ) refers to information governance as a “ rather new multidisciplinary field that is still being defined. ” Definitions or descriptions that have been put forward vary according to the perspective ( academic or professional ) and/or discipline of their authors ( for instance , records and information management , risk management and audit , law , information technology , information security ) . 
However , a thread can be discerned in terms of how the field of information governance points to the trade‐off between value and risk , and its dual function of supporting the derivation of value , and mitigation of risks of working with data . 
Research and advisory firm Gartner define information governance as : “ the specification of decision rights and an accountability framework to encourage desirable behavior in the valuation , creation , storage , use , archival [ sic ] and deletion of information . 
It includes the processes , roles , standards and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals ” ( Logan , 2010 ) . 
“ the specification of decision rights and an accountability framework to encourage desirable behavior in the valuation , creation , storage , use , archival [ sic ] and deletion of information . 
It includes the processes , roles , standards and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals ” ( Logan , 2010 ) . 
Other definitions in the professional domain focus on the processes or activities of information governance and also highlight its duality of purpose . 
For example , Hulme ( 2012 , p. 100 ) share 's IBM 's perspective as it being : “ a holistic approach to managing and using information for business benefits that encompasses information quality , information life‐cycle management , and security , privacy and compliance. ” “ a holistic approach to managing and using information for business benefits that encompasses information quality , information life‐cycle management , and security , privacy and compliance. ” This focus on business and values is perhaps most pronounced in the Information Governance Initiative ( IGI ) definition of information governance as : “ the activities and technologies that organizations employ to maximize the value of their information while minimizing associated risks and costs ” ( IGI , 2014 , p. 2 & 12 ) . 
“ the activities and technologies that organizations employ to maximize the value of their information while minimizing associated risks and costs ” ( IGI , 2014 , p. 2 & 12 ) . 
Risk , in the context of compliance , has primacy , whereas in the context of data work it is value that has primacy . 
Hence , rather than perceiving information governance as defensive and burdensome , pertaining to compliance , it is positioned as having tangible business benefits . 
This duality is also exemplified by Tallon , Short , and Harkins ( 2013 ) in their description of the evolution of information governance at the Intel Corporation from an era of “ protect ” to one of “ protect‐to‐enable. ” Focusing on how information governance can add value , Kooper , Maes , and Roos Lindgreen ( 2011 ) appear to be the only academics to have proposed a research agenda for information governance , one that is aimed at being academically rigorous and practically relevant , to explore the optimization of information value and the roles of the stakeholders ( creators , receivers , and governing actors ) . 
Drawing on the concepts of governance , corporate governance , IT/ICT governance , and data governance , they offer a comprehensive definition of information governance as : “ establishing an environment and opportunities , rules and decision‐making rights for the valuation , creation collection , analysis , destruction , storage , use and control of information ; it answers the question ‘ what information do we need , how do we make use of it and who is responsible for it ? 
’ ” ( Kooper et al. , 2011 , p. 195 ) . 
“ establishing an environment and opportunities , rules and decision‐making rights for the valuation , creation collection , analysis , destruction , storage , use and control of information ; it answers the question ‘ what information do we need , how do we make use of it and who is responsible for it ? 
’ ” ( Kooper et al. , 2011 , p. 195 ) . 
Tallon , Ramirez , and Short ( 2013 ) cite their work , along with that of Weber , Otto , and Österle ( 2009 ) and Khatri and Brown ( 2010 ) in the data governance context , in offering their definition of information governance as : “ a collection of capabilities or practices for the creation , capture , valuation , storage , usage , control , access , archival [ sic ] , and deletion of information over its life cycle ” ( Tallon et al. , 2013 , p. 142 ) . 
“ a collection of capabilities or practices for the creation , capture , valuation , storage , usage , control , access , archival [ sic ] , and deletion of information over its life cycle ” ( Tallon et al. , 2013 , p. 142 ) . 
Tallon et al. ’ s ( 2013 ) information governance model comprises three elements—structural practices ( policy , oversight , and responsibilities ) , procedural practices ( information classification , access , data protection and backup , retention and storage migration , cost ) , and relational practices ( user education , communication ) . 
Based on an extensive review of the literature and validated by a series of interviews with senior IT executives , their model ( deriving from IT governance ) also includes a series of antecedents and consequences . 
The former can enable or inhibit delivery of information governance . 
The latter represent the effects of its delivery in terms of organizational performance and risk mitigation . 
Again , this reflects the duality identified in the definitions earlier . 
Hence , while there is currently no commonly agreed definition of information governance , either in the professional or the academic domains , it is clear that common themes and vocabulary are emerging in their literatures , which could lead to consensus . 
This consensus about the definition , scope , and role of information governance appears then to be developing around three key elements : ( i ) Who is responsible , that is , who holds the decision rights and who is accountable ? 
( ii ) How is it carried out , for instance , via policy , procedures , processes , and standards for “ life cycle ” management of information ; and ( iii ) To what is it being directed , that is , “ value ” to the organization and its stakeholders , which encompasses value in the sense of compliance and risk management . 
In addition , the emerging emphasis on the duality of information governance serves to support it being a mechanism for balancing the trade‐off between the value and risks of data work . 
To operationalize this effectively in practice , frameworks or models are needed ; and a range of frameworks have emerged which , to a greater or lesser extent , reflect the who , how , and to what of information governance . 
These include : ARMA International 's Information Governance Maturity Model ( IGMM ) , based on their Generally Accepted Recordkeeping Principles ( GARP ) ( ARMA International , 2009 ) ; The Information Governance Reference Model ( IGRM ) ( EDRM.net , 2012 ) , a framework from the Information Governance Institute ( IGI , 2014 ) , and the Data Security and Protection Toolkit developed by NHS Digital , the UK National Health Service national information and technology partner ( NHS Digital https : //www.dsptoolkit.nhs.uk/ ) , which replaced the Information Governance Toolkit ( HSCIC Health & Social Care Information Centre , Department of Health https : //www.igt.hscic.gov.uk/ ) in April 2018.22 The Data Security and Protection Toolkit enables organisations that handle health and social care information to assess their performance against 10 data security standards developed by the National Data Guardian ( https : //assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/535024/data-security-review ) . 
It is mandatory for organisations that access to NHS patient data and systems to use the toolkit ) . 
The former Information Governance Toolkit was underpinned by national legislation , records management standards , and the sector‐specific Caldicott Principles , which protect patient identity ( Caldicott Review , 2013 ) . 
Information governance has been adopted as a mandate or priority for a number of professions , including records managers , health information managers , risk managers , and security specialists . 
Their respective professional literatures reflect significant coverage over the last decade , ranging from awareness‐raising , discussion of issues , and regulatory/legislative compliance , to “ rally cries ” for action and leadership and , more latterly , implementation strategies . 
It has also been covered in a number of different sectors , with the health sector being the most prominent one to feature in the literature on information governance practice ( Donaldson & Walker , 2004 ) . 
The extensive professional literature in the health sector is complemented by some academic research such as Gillies ( 2015 ) , Liaw , Pearce , Liyanage , Liaw , and de Lusignan ( 2014 ) , Renaud ( 2014 ) , and Hovenga ( 2013 ) . 
Research in other contexts includes that of Lajara and Macada ( 2013 ) in data quality , de Abreu Faria , Macada , and Kumar ( 2013 ) in banking , Silic and Back ( 2013 ) in relation to mobile devices , and Rolfe ( 2015 ) in relation to enterprise social software . 
Taking information assets ( or data ) as “ facts having value or potential value that are documented ” ( Khatri & Brown , 2010 , p. 148 ) , Khatri and Brown define data governance as “ who holds the decision rights and is held accountable for an organization 's decision making about its data assets ” ( Khatri & Brown , 2010 , p. 149 ) . 
Data governance therefore entails initial identification of who is responsible and accountable for data assets , and the structural roles and responsibilities or loci of accountability of those who realize value from them . 
These roles include a range of data and information professionals , including data owner/trustee , data quality manager , enterprise data architect/data modeler , data steward , data producer/supplier , data consumer , data security officer , and information chain manager . 
As the roles imply , there is a clear emphasis on how different roles aid in the realization of value from data in ways that are also accountable . 
How will data governance be approached ? 
As a design framework , Khatri and Brown suggest that each aspect of data governance constitutes a decision domain , and that the “ assignment of the locus of accountability for each decision domain will be somewhere on a continuum between centralized and decentralized ” ( Khatri & Brown , 2010 , p. 151 ) . 
For example , where data principles are concerned , the locus of accountability can rest with a group of corporate executives , while responsibility and decision rights for data quality may rest with the business division or unit managers , and therefore be decentralized . 
In turn , data and information professionals may be concerned with decision making in relation to accountable data access and an accountable data life cycle . 
While pointing to decision making as the main mechanism by which data are governed , Khatri and Brown ( 2010 ) also point to other structural ( for instance , standing committees ) and nonstructural mechanisms ( for instance , consistent processes , corporate announcements via web‐based portals ) , via which data are governed , and value realized . 
What is data governance directed at ? 
Inheriting their understanding from previous approaches to IT governance ( Peterson , 2004 ; Weill & Ross , 2004 , 2005 ) , Khatri and Brown suggest that there are five domains that data governance shapes . 
These domains are data principles “ clarifying the role of data as an asset ” ; data quality “ establishing the requirements of intended use of data ” ; metadata “ establishing the semantics or content of data so that it is interpretable by the users ” ; data access “ specifying access requirements of data ” ; and the data lifecycle “ determining the definition , production , retention and retirement of data ” ( Khatri & Brown , 2010 , p. 149 ) . 
