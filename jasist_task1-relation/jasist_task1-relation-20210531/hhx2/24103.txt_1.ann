T1	引文作者 225 229	Chen
T2	引文作者 232 240	Delannay
T3	引文作者 245 260	De Vleeschouwer
T4	引文作者 270 276	Mishra
T5	引文作者 279 284	Raman
T6	引文作者 287 294	Singhai
T7	引文作者 299 305	Sharma
T8	引文时间 263 267	2011
T9	引文时间 308 312	2015
T10	引文作者 953 955	Hu
T11	引文作者 958 961	Xie
T12	引文作者 964 966	Li
T13	引文作者 969 973	Zeng
T14	引文作者 978 985	Maybank
T15	引文时间 988 992	2011
T16	引文作者 1438 1442	Ejaz
T17	引文作者 1445 1452	Mehmood
T18	引文作者 1457 1461	Baik
T19	引文作者 1471 1478	Mehmood
T20	引文作者 1481 1487	Sajjad
T21	引文作者 1490 1493	Rho
T22	引文作者 1498 1502	Baik
T23	引文时间 1464 1468	2014
T24	引文时间 1505 1509	2016
T25	引文作者 1516 1523	Mehmood
T26	引文时间 1536 1540	2016
T27	引文作者 1907 1911	Gola
T28	引文作者 1914 1922	Kamiński
T29	引文作者 1925 1934	Brzezicka
T30	引文作者 1939 1945	Wróbel
T31	引文时间 1948 1952	2012
T32	引文作者 2348 2357	Moshfeghi
T33	引文作者 2360 2365	Pinto
T34	引文作者 2368 2375	Pollick
T35	引文作者 2382 2386	Jose
T36	引文时间 2389 2393	2013
T37	引文作者 3004 3012	Corbetta
T38	引文作者 3015 3022	Shulman
T39	引文时间 3025 3029	2002
T40	引文作者 3957 3962	Mayer
T41	引文时间 3965 3969	2005
T42	具体模型 4681 4684	ANN
T43	引文作者 5250 5253	Zhu
T44	引文作者 5256 5264	Goldberg
T45	引文作者 5267 5273	Eldawy
T46	引文作者 5276 5280	Dyer
T47	引文作者 5285 5291	Strock
T48	引文时间 5294 5298	2007
T49	图 5790 5991	We reviewed Mayer 's cognitive model of multimedia learning ( Figure 1 ) and Baddeley 's working memory ( WM ) theory to precisely examine the two cognitive processes ( Baddeley , 2007 ; Mayer , 2005 )
T50	引文作者 5959 5967	Baddeley
T51	引文时间 5970 5974	2007
T52	引文作者 5977 5982	Mayer
T53	引文时间 5985 5988	200
T54	图 5996 6118	As shown in Figure 1 , words and pictures in the form of a video presentation enter sensory memory via the eyes and ears .
T55	引文作者 7114 7118	West
T56	引文作者 7123 7130	Holcomb
T57	引文时间 7133 7137	2002
T58	引文作者 7744 7748	Dias
T59	引文作者 7751 7756	Sajda
T60	引文作者 7759 7769	Dmochowski
T61	引文作者 7776 7781	Parra
T62	引文时间 7784 7788	2013
T63	引文作者 8094 8104	Allegretti
T64	引文时间 8117 8121	2015
T65	引文作者 8586 8592	Kauppi
T66	引文时间 8605 8609	2015
T67	引文作者 9040 9049	Sitnikova
T68	引文作者 9052 9059	Holcomb
T69	引文作者 9062 9070	Kiyonaga
T70	引文作者 9077 9086	Kuperberg
T71	引文时间 9089 9093	2008
T72	引文作者 9829 9839	van Berkum
T73	引文作者 9842 9847	Brown
T74	引文作者 9850 9861	Zwitserlood
T75	引文作者 9864 9872	Kooijman
T76	引文作者 9877 9884	Hagoort
T77	引文作者 9894 9904	van Berkum
T78	引文作者 9907 9914	Hagoort
T79	引文作者 9919 9924	Brown
T80	引文时间 9887 9891	2005
T81	引文时间 9927 9931	1999
T82	引文作者 9938 9942	Kaan
T83	引文作者 9947 9952	Swaab
T84	引文时间 9955 9959	2003
T85	引文作者 10166 10174	Koelstra
T86	引文作者 10177 10180	Müh
T87	引文作者 10188 10194	Patras
T88	引文时间 10197 10201	2009
T89	引文作者 10427 10434	Eugster
T90	引文时间 10447 10451	2014
T91	引文作者 10932 10939	Gwizdka
T92	引文作者 10942 10950	Hosseini
T93	引文作者 10912 10918	Polich
T94	引文时间 10921 10925	2007
T95	引文时间 10971 10975	2017
T96	引文作者 10953 10957	Cole
T97	引文作者 10964 10968	Wang
T98	引文作者 11815 11829	Martín‐Loeches
T99	引文作者 11832 11840	Hinojosa
T100	引文作者 11843 11849	Casado
T101	引文作者 11852 11857	Muñoz
T102	引文作者 11862 11876	Fernández‐Frı́
T103	引文时间 11879 11883	2004
T104	引文作者 11942 11946	West
T105	引文作者 11949 11956	Holcomb
T106	引文时间 11959 11963	2002
T107	引文作者 11986 11995	Sitnikova
T108	引文时间 12005 12009	2008
T109	引文作者 12285 12295	Allegretti
T110	引文时间 12305 12309	2015
T111	引文作者 13008 13012	Hamm
T112	引文作者 13015 13022	Johnson
T113	引文作者 13027 13031	Kirk
T114	引文时间 13034 13038	2002
T115	引文作者 13531 13535	Hamm
T116	引文时间 13545 13549	2002
T117	引文作者 14103 14113	Schumacher
T118	引文作者 14116 14120	Hung
T119	引文时间 14123 14127	2012
T120	引文作者 14195 14199	Wang
T121	引文作者 14202 14212	Schumacher
T122	引文时间 14215 14219	2013
T123	引文作者 14848 14852	Hamm
T124	引文作者 14855 14862	Johnson
T125	引文作者 14867 14871	Kirk
T126	引文时间 14874 14878	2002
T127	引文作者 15371 15375	Hamm
T128	引文时间 15385 15389	2002
T129	引文作者 15943 15953	Schumacher
T130	引文作者 15956 15960	Hung
T131	引文时间 15963 15967	2012
T132	引文作者 16035 16039	Wang
T133	引文作者 16042 16052	Schumacher
T134	引文时间 16055 16059	2013
T135	表 17551 17602	Videos used for the experiment are shown in Table 1
T136	引文作者 18216 18222	Bryden
T137	引文时间 18225 18229	1982
T138	引文作者 18347 18352	Evans
T139	引文作者 18355 18358	Cui
T140	引文作者 18363 18368	Starr
T141	引文时间 18371 18375	1995
T142	图 18710 18797	Figure 2 illustrates how the videos and their frames were presented to the participants
T143	引文作者 19910 19914	Luck
T144	引文时间 19917 19921	2014
T145	引文的研究问题 1218 1435	To bridge the semantic gap , visual‐attention‐model‐based summarization schemes have been proposed to extract frames as keyframes if they are visually important for humans , as determined using visual attention models
T146	引文的研究问题 1543 1674	proposed a human‐attention model that combines both multimedia content and the viewer 's neuronal responses for video summarization
T147	引文的研究问题 2451 2577	Those researchers investigated the connection between relevance and brain activity using functional magnetic resonance imaging
T148	引文的结果 2582 2701	They observed that fronto‐parietal circuits are more activated by topic‐relevant images than by topic‐irrelevant images
T149	研究问题 3480 3675	To bridge the semantic gap , we aimed to develop a method to produce a semantically meaningful video skim by extracting topic‐relevant shots using EEG data obtained during a video‐watching sessio
T150	研究方法 4626 4761	Finally , discriminant and artificial neural network ( ANN ) analyses were used to decode video shot relevance based on the ERP signals
T151	引文的研究问题 7114 7295	West and Holcomb ( 2002 ) conducted an experiment wherein the final picture of the story was shown to each participant after viewing a series of grayscale pictures depicting a story
T152	引文的方法 7744 7949	Dias , Sajda , Dmochowski , and Parra ( 2013 ) suggested that one may predict whether a viewer would detect or miss a small target object using EEG signals acquired prior to possible fixation on the target
T153	引文的研究问题 8124 8278	conducted an ERP‐based study on the first 800 ms of a relevance assessment process to determine the time at which topic relevance is assessed in the brain
T154	引文的结果 8283 8466	The authors suggested that the central area exhibited the greatest difference between relevant and irrelevant images in the 500–800 ms time window , which corresponds to a P600 effect
T155	引文的结果 8471 8598	They additionally determined that relevance judgments for visual stimuli are made within a time window at ~800 ms. Kauppi et al
T156	引文的研究问题 8612 8834	attempted to classify visual stimuli using a Gaussian process classifier under the assumption that the relevance of an image viewed by a subject can be decoded based on magnetoencephalographic signals with high performance
T157	引文的结果 8839 9035	They additionally suggested that the fusion of gaze‐based and magnetoencephalographic‐based classifiers significantly improves the prediction performance when compared to using either signal alone
T158	引文的研究问题 9040 9154	Sitnikova , Holcomb , Kiyonaga , and Kuperberg ( 2008 ) conducted an ERP‐based experiment using silent video clips
T159	引文的结果 9159 9296	They showed that the presentation of contextually inappropriate information at the ends of videos evokes an anterior N400‐like negativity
T160	引文的结果 9301 9524	The authors further suggested that a posterior late positivity is evoked specifically when target objects presented at the ends of the videos violate goal‐related requirements of the action constrained by a scenario context
T163	引文的结果 9752 9826	A large N400 effect was elicited by discourse‐dependent semantic anomalies
T161	引文的结果 9938 10161	Kaan and Swaab ( 2003 ) suggested that a posterior P600 effect is an index of syntactic processing difficulty , while a frontal P600 effect is related to ambiguity resolution and/or an increase in discourse level complexity
T162	引文的方法 10166 10274	Koelstra , Mühl , and Patras ( 2009 ) proposed a method for automatically assigning implicit tags to a video
T164	引文的研究问题 10454 10612	conducted an experiment wherein the neural activities of 40 subjects were recorded while they assessed relevance in response to text stimuli for a given topic
T165	引文的结果 10778 10909	Thus , they concluded that P3b , which appears to be related to memory processing and retrieval , might involve relevance judgments
T166	引文的方法 10932 11109	Gwizdka , Hosseini , Cole , and Wang ( 2017 ) proposed a method to investigate processes of reading and subjective relevance judgments in search tasks using eye tracking and EEG
R1	coauthor Arg1:T1 Arg2:T2	
R2	coauthor Arg1:T2 Arg2:T3	
R3	has_cited_time Arg1:T8 Arg2:T3	
R4	coauthor Arg1:T4 Arg2:T5	
R5	coauthor Arg1:T5 Arg2:T6	
R6	coauthor Arg1:T6 Arg2:T7	
R7	has_cited_time Arg1:T9 Arg2:T7	
R8	field_similar_as Arg1:T4 Arg2:T1	
R9	coauthor Arg1:T10 Arg2:T11	
R10	coauthor Arg1:T11 Arg2:T12	
R11	coauthor Arg1:T12 Arg2:T13	
R12	coauthor Arg1:T13 Arg2:T14	
R13	has_cited_time Arg1:T15 Arg2:T14	
R14	produces Arg1:T16 Arg2:T145	
R15	coauthor Arg1:T16 Arg2:T17	
R16	coauthor Arg1:T17 Arg2:T18	
R17	has_cited_time Arg1:T23 Arg2:T18	
R18	coauthor Arg1:T19 Arg2:T20	
R19	coauthor Arg1:T20 Arg2:T21	
R20	coauthor Arg1:T21 Arg2:T22	
R21	has_cited_time Arg1:T24 Arg2:T22	
R22	field_similar_as Arg1:T19 Arg2:T16	
R23	has_cited_time Arg1:T26 Arg2:T25	
R24	produces Arg1:T25 Arg2:T146	
R25	coauthor Arg1:T27 Arg2:T28	
R26	coauthor Arg1:T28 Arg2:T29	
R27	coauthor Arg1:T29 Arg2:T30	
R28	has_cited_time Arg1:T31 Arg2:T30	
R29	coauthor Arg1:T32 Arg2:T33	
R30	coauthor Arg1:T33 Arg2:T34	
R31	coauthor Arg1:T34 Arg2:T35	
R32	has_cited_time Arg1:T36 Arg2:T35	
R33	produces Arg1:T32 Arg2:T147	
R34	coauthor Arg1:T37 Arg2:T38	
R35	has_cited_time Arg1:T39 Arg2:T38	
R36	results Arg1:T32 Arg2:T148	
R37	has_cited_time Arg1:T41 Arg2:T40	
R38	coauthor Arg1:T43 Arg2:T44	
R39	coauthor Arg1:T44 Arg2:T45	
R40	coauthor Arg1:T45 Arg2:T46	
R41	coauthor Arg1:T46 Arg2:T47	
R42	has_cited_time Arg1:T48 Arg2:T47	
R43	has_cited_time Arg1:T51 Arg2:T50	
R44	has_cited_time Arg1:T53 Arg2:T52	
R45	field_similar_as Arg1:T52 Arg2:T50	
R46	coauthor Arg1:T55 Arg2:T56	
R47	has_cited_time Arg1:T57 Arg2:T56	
R48	produces Arg1:T55 Arg2:T151	
R49	coauthor Arg1:T58 Arg2:T59	
R50	coauthor Arg1:T59 Arg2:T60	
R51	coauthor Arg1:T60 Arg2:T61	
R52	has_cited_time Arg1:T62 Arg2:T61	
R53	uses Arg1:T58 Arg2:T152	
R54	has_cited_time Arg1:T64 Arg2:T63	
R55	produces Arg1:T63 Arg2:T153	
R56	results Arg1:T63 Arg2:T154	
R57	results Arg1:T63 Arg2:T155	
R58	has_cited_time Arg1:T66 Arg2:T65	
R59	produces Arg1:T65 Arg2:T156	
R60	results Arg1:T65 Arg2:T157	
R61	coauthor Arg1:T67 Arg2:T68	
R62	coauthor Arg1:T68 Arg2:T69	
R63	coauthor Arg1:T69 Arg2:T70	
R64	has_cited_time Arg1:T71 Arg2:T70	
R65	produces Arg1:T67 Arg2:T158	
R66	results Arg1:T67 Arg2:T159	
R67	results Arg1:T67 Arg2:T160	
R68	results Arg1:T72 Arg2:T163	
R69	coauthor Arg1:T72 Arg2:T73	
R70	coauthor Arg1:T73 Arg2:T74	
R71	coauthor Arg1:T74 Arg2:T75	
R72	coauthor Arg1:T75 Arg2:T76	
R73	has_cited_time Arg1:T80 Arg2:T76	
R74	coauthor Arg1:T77 Arg2:T78	
R75	coauthor Arg1:T78 Arg2:T79	
R76	has_cited_time Arg1:T81 Arg2:T79	
R77	field_similar_as Arg1:T77 Arg2:T72	
R78	coauthor Arg1:T82 Arg2:T83	
R79	has_cited_time Arg1:T84 Arg2:T83	
R80	results Arg1:T82 Arg2:T161	
R81	coauthor Arg1:T85 Arg2:T86	
R82	coauthor Arg1:T86 Arg2:T87	
R83	has_cited_time Arg1:T88 Arg2:T87	
R84	uses Arg1:T85 Arg2:T162	
R85	has_cited_time Arg1:T90 Arg2:T89	
R86	produces Arg1:T89 Arg2:T164	
R87	has_cited_time Arg1:T94 Arg2:T93	
R88	results Arg1:T93 Arg2:T165	
R89	coauthor Arg1:T91 Arg2:T92	
R90	coauthor Arg1:T92 Arg2:T96	
R91	coauthor Arg1:T96 Arg2:T97	
R92	has_cited_time Arg1:T95 Arg2:T97	
R93	uses Arg1:T91 Arg2:T166	
R94	coauthor Arg1:T98 Arg2:T99	
R95	coauthor Arg1:T99 Arg2:T100	
R96	coauthor Arg1:T100 Arg2:T101	
R97	coauthor Arg1:T101 Arg2:T102	
R98	has_cited_time Arg1:T103 Arg2:T102	
R99	coauthor Arg1:T104 Arg2:T105	
R100	has_cited_time Arg1:T106 Arg2:T105	
R101	has_cited_time Arg1:T108 Arg2:T107	
R102	coauthor Arg1:T111 Arg2:T112	
R103	coauthor Arg1:T112 Arg2:T113	
R104	has_cited_time Arg1:T114 Arg2:T113	
R105	has_cited_time Arg1:T116 Arg2:T115	
R106	coauthor Arg1:T117 Arg2:T118	
R107	has_cited_time Arg1:T119 Arg2:T118	
R108	coauthor Arg1:T120 Arg2:T121	
R109	has_cited_time Arg1:T122 Arg2:T121	
R110	coauthor Arg1:T123 Arg2:T124	
R111	coauthor Arg1:T124 Arg2:T125	
R112	has_cited_time Arg1:T126 Arg2:T125	
R113	has_cited_time Arg1:T128 Arg2:T127	
R114	coauthor Arg1:T129 Arg2:T130	
R115	has_cited_time Arg1:T131 Arg2:T130	
R116	coauthor Arg1:T132 Arg2:T133	
R117	has_cited_time Arg1:T134 Arg2:T133	
R118	has_cited_time Arg1:T137 Arg2:T136	
R119	coauthor Arg1:T138 Arg2:T139	
R120	coauthor Arg1:T139 Arg2:T140	
R121	has_cited_time Arg1:T141 Arg2:T140	
R122	has_cited_time Arg1:T144 Arg2:T143	
