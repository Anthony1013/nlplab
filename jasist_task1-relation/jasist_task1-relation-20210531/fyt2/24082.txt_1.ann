T1	引文时间 893 897	2017
T2	引文作者 875 890	Gómez‐Rodríguez
T3	引文作者 864 870	Alonso
T4	引文作者 854 861	Vilares
T5	引文作者 807 818	Ratnaparkhi
T6	引文时间 821 825	1996
T7	引文作者 771 781	De Meulder
T8	引文作者 754 768	Tjong Kim Sang
T9	引文时间 784 788	2003
T10	引文作者 2537 2542	Doval
T11	引文作者 2545 2552	Vilares
T12	引文作者 2557 2572	Gómez‐Rodríguez
T13	引文时间 2575 2579	2015
T14	引文作者 2890 2897	Thurlow
T15	引文作者 2900 2905	Brown
T16	引文时间 2908 2912	2003
T17	表 1654 1735	In Table 1 we show some example instances of the problem we are trying to solve .
T18	引文作者 4003 4006	Kim
T19	引文作者 4009 4016	Jernite
T20	引文作者 4019 4025	Sontag
T21	引文作者 4030 4034	Rush
T22	引文时间 4037 4041	2015
T23	引文作者 4508 4512	Wang
T24	引文作者 4515 4523	Thrasher
T25	引文作者 4528 4531	Hsu
T26	引文时间 4534 4538	2011
T27	引文作者 4993 5000	Manning
T28	引文作者 5003 5011	Raghavan
T29	引文作者 5016 5023	Schütze
T30	引文时间 5026 5030	2008
T31	引文作者 5756 5761	Koehn
T32	引文作者 5764 5770	Knight
T33	引文时间 5773 5777	2003
T34	引文作者 5806 5815	Alfonseca
T35	引文作者 5818 5823	Bilac
T36	引文作者 5828 5835	Pharies
T37	引文时间 5838 5842	2008
T38	引文作者 5871 5882	Adda‐Decker
T39	引文作者 5885 5889	Adda
T40	引文作者 5894 5899	Lamel
T41	引文时间 5902 5906	2000
T42	引文作者 6256 6260	Chen
T43	引文作者 6263 6266	Qiu
T44	引文作者 6269 6272	Zhu
T45	引文作者 6275 6278	Liu
T46	引文作者 6283 6288	Huang
T47	引文时间 6291 6295	2015
T48	引文作者 6298 6301	Pei
T49	引文作者 6304 6306	Ge
T50	引文作者 6311 6316	Chang
T51	引文时间 6319 6323	2014
T52	引文作者 6326 6328	Xu
T53	引文作者 6331 6334	Sun
T54	引文时间 6337 6341	2016
T55	引文作者 6344 6349	Zheng
T56	引文作者 6352 6356	Chen
T57	引文作者 6361 6363	Xu
T58	引文时间 6366 6370	2013
T59	引文作者 6620 6625	Koehn
T60	引文作者 6594 6603	Alfonseca
T61	引文时间 6613 6617	2008
T62	引文作者 6628 6634	Knight
T63	引文时间 6637 6641	2003
T64	引文作者 6721 6726	Krott
T65	引文作者 6729 6738	Schreuder
T66	引文作者 6741 6754	Harald Baayen
T67	引文作者 6759 6767	Dressler
T68	引文时间 6770 6774	2007
T69	引文作者 7476 7479	Chi
T70	引文作者 7482 7486	Ding
T71	引文作者 7491 7494	Lim
T72	引文时间 7497 7501	1999
T73	引文作者 7504 7508	Wang
T74	引文时间 7518 7522	2011
T75	引文作者 8073 8080	Maynard
T76	引文作者 8083 8092	Greenwood
T77	引文时间 8095 8099	2014
T78	引文作者 8102 8112	Srinivasan
T79	引文作者 8115 8127	Bhattacharya
T80	引文作者 8132 8143	Chakraborty
T81	引文时间 8146 8150	2012
T82	引文作者 8727 8732	Koehn
T83	引文作者 8735 8741	Knight
T84	引文时间 8744 8748	2003
T85	引文作者 8823 8832	Kacmarcik
T86	引文作者 8835 8843	Brockett
T87	引文作者 8848 8854	Suzuki
T88	引文时间 8857 8861	2000
T89	引文作者 9241 9245	Wang
T90	引文时间 9255 9259	2011
T91	引文作者 9157 9159	Wu
T92	引文作者 9162 9167	Jiang
T93	引文时间 9170 9174	1998
T94	引文作者 9389 9392	Xue
T95	引文时间 9395 9399	2003
T96	引文作者 9913 9916	Low
T97	引文作者 9919 9921	Ng
T98	引文作者 9926 9929	Guo
T99	引文时间 9932 9936	2005
T100	引文作者 9971 9975	Peng
T101	引文作者 9978 9982	Feng
T102	引文作者 9987 9995	McCallum
T103	引文时间 9998 10002	2004
T104	引文作者 10086 10090	Chen
T105	引文作者 10093 10096	Qiu
T106	引文作者 10099 10102	Zhu
T107	引文作者 10107 10112	Huang
T108	引文时间 10115 10119	2015
T109	引文作者 10122 10125	Pei
T110	引文时间 10135 10139	2014
T111	引文作者 10142 10147	Zheng
T112	引文时间 10157 10161	2013
T113	引文作者 10257 10261	Zhao
T114	引文作者 10249 10252	Cai
T115	引文时间 10264 10268	2016
T116	引文作者 10412 10417	Zhang
T117	引文作者 10420 10425	Zhang
T118	引文作者 10432 10434	Fu
T119	引文时间 10437 10441	2016
T120	引文作者 11053 11057	Wang
T121	引文时间 11067 11071	2011
T122	引文时间 11717 11721	2010
T123	引文作者 11711 11714	Hsu
T124	引文作者 11702 11704	Li
T125	引文作者 11693 11699	Viegas
T126	引文作者 11682 11690	Thrasher
T127	引文作者 11675 11679	Wang
T128	引文作者 11817 11821	Wang
T129	引文作者 11824 11826	Li
T130	引文时间 11829 11833	2009
T131	引文时间 14036 14040	2009
T132	引文作者 14027 14033	Norvig
T133	引文时间 14708 14712	2016
T134	引文作者 14698 14705	Vilares
T135	引文作者 14676 14691	Gómez‐Rodríguez
T136	引文作者 14668 14673	Doval
T137	引文作者 16781 16788	Mikolov
T138	引文作者 16791 16796	Zweig
T139	引文时间 16799 16803	2012
T140	引文作者 16828 16836	Heafield
T141	引文作者 16839 16850	Pouzyrevsky
T142	引文作者 16853 16858	Clark
T143	引文作者 16863 16868	Koehn
T144	引文作者 17064 17067	Kuo
T145	引文时间 17070 17074	1993
T146	引文时间 17483 17487	2017
T147	引文作者 17465 17480	Gómez‐Rodríguez
T148	引文作者 17455 17462	Vilares
T149	引文作者 17513 17518	Iyyer
T150	引文作者 17521 17532	Boyd‐Graber
T151	引文作者 17535 17543	Claudino
T152	引文作者 17546 17552	Socher
T153	引文作者 17557 17566	Daumé III
T154	引文时间 17569 17573	2014
T155	引文作者 17408 17415	Johnson
T156	引文时间 17425 17429	2016
T157	引文作者 17051 17059	Principe
T158	引文作者 17601 17612	Sundermeyer
T159	引文作者 17615 17623	Schlüter
T160	引文作者 17628 17631	Ney
T161	引文时间 17634 17638	2012
T162	引文作者 18033 18043	Hochreiter
T163	引文作者 18046 18057	Schmidhuber
T164	引文时间 18060 18064	1997
T165	引文作者 18172 18183	Sundermeyer
T166	引文时间 18193 18197	2012
T167	引文作者 19696 19704	Heafield
T168	引文时间 19714 19718	2013
T169	引文作者 19721 19727	Kneser
T170	引文作者 19730 19733	Ney
T171	引文时间 19736 19740	1995
T172	引文作者 19781 19785	Chen
T173	引文作者 19788 19795	Goodman
T174	引文时间 19798 19802	1996
T175	引文作者 20566 20573	Zaremba
T176	引文作者 20231 20234	Kim
T177	引文时间 20244 20248	2015
T178	引文作者 20576 20585	Sutskever
T179	引文作者 20590 20597	Vinyals
T180	引文时间 20600 20604	2014
T181	引文作者 22585 22591	Kingma
T182	引文作者 22594 22596	Ba
T183	引文时间 22599 22603	2014
T184	引文作者 22491 22498	Léonard
T185	引文作者 22501 22509	Waghmare
T186	引文作者 22512 22516	Wang
T187	引文作者 22521 22524	Kim
T188	引文时间 22527 22531	2015
T189	具体模型 1913 1925	n‐gram model
T190	研究方法 1738 1940	Our proposed approach for word segmentation uses a beam search algorithm aided by a byte or character level language model , which is implemented using a neural network or an n‐gram model , respectively
T191	研究方法 1949 1970	beam search algorithm
T192	研究方法 3459 3737	As it would be highly improbable to observe every standard word affected by every single type of texting phenomena , we abandon the word‐level processing of these texts and opt instead for a character or byte level approach in order to tackle the resulting data sparsity problem
T193	研究结果 5049 5252	Overall , our approach was able to outperform both the Word Breaker and WordSegment for all of the languages considered , with the sole exception of a tie with WordSegment in one of the Spanish data sets
T194	引文的研究问题 6103 6159	the application of word segmentation for these languages
T195	引文的方法 9895 9910	Maximum Entropy
T196	引文的方法 9943 9968	Conditional Random Fields
T199	术语 11801 11814	CALM algoritm
T200	数据源 11870 11916	Bing search engine.22 https : //www.bing.com /
T201	具体模型 12385 12417	word‐based n‐gram language model
T202	具体模型 13440 13452	n‐gram model
T203	术语 14145 14162	Viterbi algorithm
T205	研究方法 15400 15500	Our proposed approach is formed by two components : The beam search algorithm and the language model
T206	研究方法 15505 15592	The search algorithm acquires the input text and first removes all word boundary tokens
T207	研究方法 15597 15731	Then it analyzes the resulting text one token at a time , deciding whether a word boundary token would be appropriate in that position
T208	研究方法 15736 15831	If it is , two partial segmentation candidates may be generated , with and without the boundary
T209	研究方法 15836 15994	At some point , the number of candidates exceeds some predefined upper limit n , the beam width , and the n best candidates are chosen to continue the process
T210	研究方法 15999 16122	When the whole input is processed by this algorithm , m candidates from the currently n best are chosen as the final result
T211	图 16127 16190	In Figure 1 we show a simplification of the described procedure
T213	研究方法 18204 18400	These LSTM units contain a memory cell , which stores information from past computations , and three gates that control the information stored in the memory as well as the output of the whole unit
T214	研究方法 18405 18556	Using a byte‐level approach we can reuse the same network design for multiple languages , as the character set is not a parameter in the design process
T215	研究方法 19809 20002	We avoid the data sparsity problem mentioned in the Introduction by using language models that work at the byte and character level instead of the word level , and also by using neural networks
T216	引文作者 20713 20718	Ioffe
T217	引文作者 20721 20728	Szegedy
T218	引文时间 20731 20735	2015
T219	术语 22555 22582	Adam optimization algorithm
T220	时间 23215 23219	2015
T197	引文的方法 10271 10407	obtain segmented word embeddings from the corresponding candidate character sequences and then feed them to a neural network for scoring
T198	引文的方法 10444 10702	consider a transition‐based framework where they process the input at the character level and use neural networks to decide on the next action given the current state of the system : Append the character to a previous segmented word or insert a word boundary
T204	引文的方法 14715 14806	with a new beam search algorithm and newer implementations for the language model component
T212	研究方法 18020 18030	LSTM units
R1	coauthor Arg1:T8 Arg2:T7	
R2	has_cited_time Arg1:T9 Arg2:T7	
R3	has_cited_time Arg1:T6 Arg2:T5	
R4	coauthor Arg1:T4 Arg2:T3	
R5	coauthor Arg1:T3 Arg2:T2	
R6	has_cited_time Arg1:T1 Arg2:T2	
R7	coauthor Arg1:T10 Arg2:T11	
R8	coauthor Arg1:T11 Arg2:T12	
R9	has_cited_time Arg1:T13 Arg2:T12	
R10	coauthor Arg1:T14 Arg2:T15	
R11	has_cited_time Arg1:T16 Arg2:T15	
R12	coauthor Arg1:T18 Arg2:T19	
R13	coauthor Arg1:T19 Arg2:T20	
R14	coauthor Arg1:T20 Arg2:T21	
R15	has_cited_time Arg1:T22 Arg2:T21	
R16	coauthor Arg1:T23 Arg2:T24	
R17	coauthor Arg1:T24 Arg2:T25	
R18	has_cited_time Arg1:T26 Arg2:T25	
R19	coauthor Arg1:T27 Arg2:T28	
R20	coauthor Arg1:T28 Arg2:T29	
R21	has_cited_time Arg1:T30 Arg2:T29	
R22	coauthor Arg1:T31 Arg2:T32	
R23	has_cited_time Arg1:T33 Arg2:T32	
R24	coauthor Arg1:T34 Arg2:T35	
R25	coauthor Arg1:T35 Arg2:T36	
R26	has_cited_time Arg1:T37 Arg2:T36	
R27	coauthor Arg1:T38 Arg2:T39	
R28	coauthor Arg1:T39 Arg2:T40	
R29	has_cited_time Arg1:T41 Arg2:T40	
R30	produces Arg1:T42 Arg2:T194	
R31	coauthor Arg1:T42 Arg2:T43	
R32	coauthor Arg1:T43 Arg2:T44	
R33	coauthor Arg1:T44 Arg2:T45	
R34	has_cited_time Arg1:T47 Arg2:T46	
R35	coauthor Arg1:T45 Arg2:T46	
R36	coauthor Arg1:T48 Arg2:T49	
R37	coauthor Arg1:T49 Arg2:T50	
R38	has_cited_time Arg1:T51 Arg2:T50	
R39	coauthor Arg1:T52 Arg2:T53	
R40	has_cited_time Arg1:T54 Arg2:T53	
R41	has_cited_time Arg1:T58 Arg2:T57	
R42	coauthor Arg1:T55 Arg2:T56	
R43	coauthor Arg1:T56 Arg2:T57	
R44	field_similar_as Arg1:T42 Arg2:T48	
R45	field_similar_as Arg1:T48 Arg2:T55	
R46	has_cited_time Arg1:T61 Arg2:T60	
R47	coauthor Arg1:T59 Arg2:T62	
R48	has_cited_time Arg1:T63 Arg2:T62	
R49	coauthor Arg1:T64 Arg2:T65	
R50	coauthor Arg1:T65 Arg2:T66	
R51	coauthor Arg1:T66 Arg2:T67	
R52	has_cited_time Arg1:T68 Arg2:T67	
R53	coauthor Arg1:T69 Arg2:T70	
R54	coauthor Arg1:T70 Arg2:T71	
R55	has_cited_time Arg1:T72 Arg2:T71	
R56	coauthor Arg1:T69 Arg2:T73	
R57	has_cited_time Arg1:T74 Arg2:T73	
R58	coauthor Arg1:T75 Arg2:T76	
R59	has_cited_time Arg1:T77 Arg2:T76	
R60	coauthor Arg1:T78 Arg2:T79	
R61	coauthor Arg1:T79 Arg2:T80	
R62	has_cited_time Arg1:T81 Arg2:T80	
R63	field_similar_as Arg1:T75 Arg2:T78	
R64	coauthor Arg1:T82 Arg2:T83	
R65	has_cited_time Arg1:T84 Arg2:T83	
R66	coauthor Arg1:T85 Arg2:T86	
R67	coauthor Arg1:T86 Arg2:T87	
R68	has_cited_time Arg1:T88 Arg2:T87	
R69	coauthor Arg1:T91 Arg2:T92	
R70	has_cited_time Arg1:T93 Arg2:T92	
R71	has_cited_time Arg1:T90 Arg2:T89	
R72	has_cited_time Arg1:T95 Arg2:T94	
R73	uses Arg1:T96 Arg2:T195	
R74	coauthor Arg1:T96 Arg2:T97	
R75	coauthor Arg1:T97 Arg2:T98	
R76	has_cited_time Arg1:T99 Arg2:T98	
R77	uses Arg1:T100 Arg2:T196	
R78	coauthor Arg1:T100 Arg2:T101	
R79	coauthor Arg1:T101 Arg2:T102	
R80	has_cited_time Arg1:T103 Arg2:T102	
R81	coauthor Arg1:T104 Arg2:T105	
R82	coauthor Arg1:T105 Arg2:T106	
R83	coauthor Arg1:T106 Arg2:T107	
R84	has_cited_time Arg1:T108 Arg2:T107	
R85	has_cited_time Arg1:T110 Arg2:T109	
R86	has_cited_time Arg1:T112 Arg2:T111	
R87	field_similar_as Arg1:T104 Arg2:T109	
R88	field_similar_as Arg1:T109 Arg2:T111	
R89	coauthor Arg1:T114 Arg2:T113	
R90	has_cited_time Arg1:T115 Arg2:T113	
R91	uses Arg1:T113 Arg2:T197	
R92	coauthor Arg1:T116 Arg2:T117	
R93	coauthor Arg1:T117 Arg2:T118	
R94	has_cited_time Arg1:T119 Arg2:T118	
R95	uses Arg1:T116 Arg2:T198	
R96	has_cited_time Arg1:T121 Arg2:T120	
R97	coauthor Arg1:T127 Arg2:T126	
R98	coauthor Arg1:T126 Arg2:T125	
R99	coauthor Arg1:T125 Arg2:T124	
R100	coauthor Arg1:T124 Arg2:T123	
R101	has_cited_time Arg1:T122 Arg2:T123	
R102	coauthor Arg1:T128 Arg2:T129	
R103	has_cited_time Arg1:T130 Arg2:T129	
R104	has_cited_time Arg1:T131 Arg2:T132	
R105	coauthor Arg1:T136 Arg2:T135	
R106	coauthor Arg1:T135 Arg2:T134	
R107	has_cited_time Arg1:T133 Arg2:T134	
R108	uses Arg1:T136 Arg2:T204	
R109	coauthor Arg1:T137 Arg2:T138	
R110	has_cited_time Arg1:T139 Arg2:T138	
R111	coauthor Arg1:T140 Arg2:T141	
R112	coauthor Arg1:T141 Arg2:T142	
R113	coauthor Arg1:T142 Arg2:T143	
T221	引文时间 16871 16875	2013
R114	has_cited_time Arg1:T221 Arg2:T143	
R115	coauthor Arg1:T157 Arg2:T144	
R116	has_cited_time Arg1:T145 Arg2:T144	
R117	has_cited_time Arg1:T146 Arg2:T147	
R118	coauthor Arg1:T149 Arg2:T150	
R119	coauthor Arg1:T150 Arg2:T151	
R120	coauthor Arg1:T151 Arg2:T152	
R121	coauthor Arg1:T152 Arg2:T153	
R122	has_cited_time Arg1:T154 Arg2:T153	
R123	has_cited_time Arg1:T156 Arg2:T155	
R124	coauthor Arg1:T148 Arg2:T147	
R125	has_cited_time Arg1:T164 Arg2:T163	
R126	coauthor Arg1:T162 Arg2:T163	
R127	coauthor Arg1:T158 Arg2:T159	
R128	coauthor Arg1:T159 Arg2:T160	
R129	has_cited_time Arg1:T161 Arg2:T160	
R130	has_cited_time Arg1:T166 Arg2:T165	
R131	has_cited_time Arg1:T168 Arg2:T167	
R132	coauthor Arg1:T169 Arg2:T170	
R133	has_cited_time Arg1:T171 Arg2:T170	
R134	coauthor Arg1:T172 Arg2:T173	
R135	has_cited_time Arg1:T174 Arg2:T173	
R136	coauthor Arg1:T175 Arg2:T178	
R137	coauthor Arg1:T178 Arg2:T179	
R138	has_cited_time Arg1:T180 Arg2:T179	
R139	has_cited_time Arg1:T177 Arg2:T176	
R140	coauthor Arg1:T216 Arg2:T217	
R141	has_cited_time Arg1:T218 Arg2:T217	
R142	coauthor Arg1:T184 Arg2:T185	
R143	coauthor Arg1:T185 Arg2:T186	
R144	coauthor Arg1:T186 Arg2:T187	
R145	has_cited_time Arg1:T188 Arg2:T187	
R146	coauthor Arg1:T181 Arg2:T182	
R147	has_cited_time Arg1:T183 Arg2:T182	
